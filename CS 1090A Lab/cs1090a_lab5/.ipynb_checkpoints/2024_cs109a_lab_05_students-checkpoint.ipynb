{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4,
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# \u003cimg style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"\u003e CS1090A Introduction to Data Science \n",
                "\n",
                "## Lab 5: Cross-validation, Bootstrap, and Regularization \n",
                "\n",
                "**Harvard University**\u003cbr/\u003e\n",
                "**Fall 2024**\u003cbr/\u003e\n",
                "**Instructors**: Pavlos Protopapas, Natesh Pillai, and Chris Gumb\u003cbr/\u003e\n",
                "\u003chr style='height:2px'\u003e"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Material preparation**: Robert Roessler, Queenie Luo, and Chris Gumb"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Data and Stats packages\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "#pd.set_option('max_columns', 200)\n",
                "\n",
                "# Visualization packages\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "sns.set()\n",
                "\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "from sklearn.linear_model import LinearRegression\n",
                "from sklearn.neighbors import KNeighborsRegressor\n",
                "from sklearn.model_selection import train_test_split, cross_validate\n",
                "from sklearn import preprocessing\n",
                "from sklearn.metrics import mean_squared_error, r2_score\n",
                "#from prettytable import PrettyTable"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import RidgeCV\n",
                "from sklearn.linear_model import LassoCV\n",
                "from sklearn.linear_model import Ridge\n",
                "from sklearn.linear_model import Lasso\n",
                "import sys"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Some global matplotlib settings\n",
                "plt.rcParams.update({\n",
                "    \"text.usetex\": False,\n",
                "    \"font.family\": \"serif\",\n",
                "    \"font.sans-serif\": [\"Times New Roman\"],\n",
                "    \"figure.dpi\" : 100})\n",
                "\n",
                "# to display nice pipeline drawing\n",
                "from sklearn import set_config\n",
                "set_config(display=\"diagram\") # instead of display='text'\n",
                "pd.set_option('display.max_columns', None)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003ca id=top\u003e\u003c/a\u003e\n",
                "### Lab Overview\n",
                "\n",
                "- [1 - Cross Validation](#cv)\u003cBR\u003e\n",
                "- [2 - Bootstrapping](#boot)\u003cBR\u003e\n",
                "- [3 - Confidence Intervals](#ci)\u003cBR\u003e\n",
                "- [4 - Feature importance](#feature)\u003cBR\u003e\n",
                "- [5 - Linear Regression Coefficient interpretation](#coeff_interpretation)\u003cBR\u003e\n",
                "- [6 - Model Selection review with an example of ridge and lasso regression using cross validation.](#model)\u003cBR\u003e"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### The dataset\n",
                "We are revisiting the dataset from last week's lab, which focused on Premier League Soccer Data. With this dataset, our primary objective was to predict the market value of Premier League players, denominated in millions of pounds, based on a variety of features.\n",
                "\n",
                "In the previous lab, you learned about the critical preparatory steps before model training. This involved feature engineering, where transformations such as `OneHotEncoder()` might be applied to handle categorical data. We also introduced the `PolynomialFeatures()` method, which allowed us to generate higher-degree features as well as interaction terms, capturing relationships between different features. Most importantly, we emphasized the importance of standardizing the features using techniques like the `StandardScaler()` to ensure that our model treats all features on a common scale. The data we're loading now is the outcome of those efforts."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Loading the data\n",
                "design_train_df = pd.read_csv(\"data/design_train_df.csv\")\n",
                "y_train = pd.read_csv(\"data/y_train.csv\")  # you could add .values.ravel() to make sure y_train is a 1D array\n",
                "\n",
                "design_test_df = pd.read_csv(\"data/design_test_df.csv\")\n",
                "y_test = pd.read_csv(\"data/y_test.csv\")\n",
                "\n",
                "display(design_train_df.head(5))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv id=cv style=\"border: thin solid black; background: lightsalmon; padding: 5px\"\u003e\n",
                "\u003ch1\u003e1 - Cross Validation\u003c/h1\u003e \n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Intro\n",
                "\n",
                "Cross-validation is a technique for evaluating predictive models. In k-fold cross-validation, the dataset is divided into k subsets or folds. The model is trained and evaluated k times, using a different fold as the validation set each time. Performance metrics from each fold are averaged to estimate the model's generalization performance.\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cimg src=\"fig/cross_val.png\" alt=\"alt text\" width=\"500\" height=\"300\" class=\"blog-image\"\u003e\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "---\n",
                "\n",
                "### Key Points for Cross Validation\n",
                "\n",
                "\u003cdiv style=\"padding-top:0px;\"\u003e\u003c/div\u003e\n",
                "\n",
                "1. **Role of Training, Validation, and Test Sets:**\n",
                "    - **Key Takeaway:** The training set is used to fit candidate models, their performance on the validation set is used to select from among candidates, and the selected model's performance is finally evaluated on the test set.\n",
                "    - **Rationale:** Use of a validation set for model selection prevents overfitting to the training data. A separate test set helps us estimate the selected model's performance on previously unseen data.\n",
                "\n",
                "\u003cdiv style=\"padding-top:0px;\"\u003e\u003c/div\u003e\n",
                "\n",
                "2. **$k$-Fold Cross Validation:**\n",
                "    - **Key Takeaway:** One common method of model selection is $k$-Fold cross-validation where the original training data is partitioned into $k$ subsets or 'folds.' The model is trained on $k$-1 folds and validated on the remaining one. This process is repeated K times.\n",
                "    - **Rationale:** By rotating the validation set through all data points, we can get a more robust measure of model performance.\n",
                "\n",
                "\u003cdiv style=\"padding-top:0px;\"\u003e\u003c/div\u003e\n",
                "\n",
                "3. **Choosing the Right Number of Folds:**\n",
                "    - **Key Takeaway:** The choice of $k$ in $k$-Fold cross-validation is crucial. Common choices are 5 or 10, but the optimal number may vary depending on the dataset size and specific problem.\n",
                "    - **Rationale:** A smaller $k$ will result in larger validation sets, which gives more accurate estimates of model performance but with higher variance. A larger $k$ provides smaller validation sets, leading to a less accurate model performance estimate but with lower variance.\n",
                "\n",
                "---\n",
                "\n",
                "### Manual Implementation of k-fold Cross-Validation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# K-Fold cross-validation\n",
                "\n",
                "# Number of partitions/folds to divide the dataset into\n",
                "k_folds = 8\n",
                "\n",
                "# Calculate the size of each fold. If the dataset's length isn't \n",
                "# perfectly divisible by k, some folds might have an extra data point.\n",
                "fold_size = len(design_train_df) // k_folds\n",
                "\n",
                "# This list will store the mean squared error for each fold.\n",
                "val_mses = []\n",
                "\n",
                "# Why might we want to shuffle our data first?\n",
                "X_train = design_train_df.sample(frac=1, replace=False, random_state=42).copy() # frac=1 effectively shuffles\n",
                "y_train = y_train.iloc[X_train.index] # sort y by indices of shuffled X\n",
                "X_train = X_train.reset_index(drop=True)\n",
                "y_train = y_train.reset_index(drop=True)\n",
                "\n",
                "# Iterate over each fold\n",
                "for i in range(k_folds):\n",
                "\n",
                "    # Compute the starting and ending indices for the test set based on the current fold.\n",
                "    # For example, if fold_size is 100:\n",
                "    # i=0 -\u003e start=0, end=100\n",
                "    # i=1 -\u003e start=100, end=200\n",
                "    # ... and so on\n",
                "    start, end = i * fold_size, (i + 1) * fold_size\n",
                "\n",
                "    # Use slicing to get the test set for the current fold from the main dataset.\n",
                "    # your code here\n",
                "    X_val_fold = X_train[...]\n",
                "    y_val_fold = ...\n",
                "\n",
                "    # For the training set, we take all data EXCEPT the test fold. This is achieved\n",
                "    # by concatenating the two slices of data that are before and after the test fold.\n",
                "    # Note: we are using `pd.concat` for DataFrames instead of `np.concatenate`.\n",
                "    X_train_folds = pd.concat([X_train[:start], X_train[end:]])\n",
                "    y_train_folds = np.concatenate([y_train[:start], y_train[end:]])\n",
                "\n",
                "    # Initialize a linear regression model\n",
                "    model = LinearRegression()\n",
                "    \n",
                "    # Train the model using the training set\n",
                "    # your code here\n",
                "    model.fit(..., ...)\n",
                "    \n",
                "    # Predict the target values for the validation set\n",
                "    # your code here\n",
                "    y_hat_val_fold = model.predict(...)\n",
                "\n",
                "    # Compute the mean squared error for the current fold's predictions\n",
                "    # your code here\n",
                "    mse = mean_squared_error(...)\n",
                "\n",
                "    # Store the computed MSE to our list\n",
                "    val_mses.append(mse)\n",
                "\n",
                "# Calculate the average mean squared error across all folds.\n",
                "avg_mse = np.mean(val_mses)\n",
                "\n",
                "# Print the result\n",
                "print(f\"Average MSE from simplified scratch implementation: {avg_mse:.2f}\")\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "## load solutions if stuck\n",
                "# %load snips/k-fold.py"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.bar(np.arange(1,len(val_mses)+1), val_mses)\n",
                "plt.axhline(avg_mse, ls='-', lw=4, c='r', label=\"Mean MSE Across Folds\")\n",
                "plt.title(f\"{k_folds}-fold Validation\")\n",
                "plt.xlabel(\"Validation Fold\")\n",
                "plt.ylabel(\"MSE\")\n",
                "plt.legend();"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### K-fold Cross-Validation with scikit-learn's 'cross_validate()'\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Using cross_validate function on training data\n",
                "model = LinearRegression()\n",
                "scores = cross_validate(estimator=model,\n",
                "                        X=X_train,\n",
                "                        y=y_train,\n",
                "                        cv=k_folds,\n",
                "                        scoring='neg_mean_squared_error', # negative MSE because it maximizes\n",
                "                        return_estimator=False,\n",
                "                        return_train_score=False) \n",
                "\n",
                "# Convert negative MSE back to positive\n",
                "avg_mse_sklearn = -np.mean(scores['test_score'])  # Despite the name, this is the validation score!!!\n",
                "print(f\"Average MSE using scikit-learn: {avg_mse_sklearn:.2f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# cross_validate returns a dictionary with values from each fold\n",
                "scores"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "### You might be wondering:\n",
                "\n",
                "**\"why neg_mean_squared_error\"?**\n",
                "  \n",
                "*Answer:* In Scikit-learn, a general principle is that greater values should always correspond to better outcomes. This makes sense when thinking about scores like accuracy or R^2 where higher values are obviously better. However, when it comes to loss functions like Mean Squared Error (MSE), a lower value is better. So, to make it fit into Scikit-learn's general principle, the negative of the MSE is used. This way, larger values (which are less negative) still indicate better models. This principle is embedded in Scikit-learn's unified scoring API. \n",
                "When you perform hyperparameter tuning using something like GridSearchCV or RandomizedSearchCV, Scikit-learn tries to maximize the score. If you used a positive MSE, the optimization routine would erroneously try to maximize the MSE, which isn't what we want. By using a negative MSE, the optimization correctly tries to find a model that gets the \"maximum\" score, which corresponds to the minimum actual MSE.\n",
                "\n",
                "**\"Why use K-Fold and not e.g. LPOCV or LOOCV (Leave-P-Out or Leave-One-Out Cross Validation)?\"**\n",
                "\n",
                "*Answer:* LOOCV can be computationally expensive for large datasets, as it involves training a model n times (where n is the number of data points). K-Fold, especially with k=5 or 10, provides a good balance between computational cost and reliable estimation of model performance.\n",
                "\n",
                "**In the manual cross-validation example, why didn't we shuffle the data?**\n",
                "\n",
                "*Answer:* The manual example was a simplified demonstration. In many real-world scenarios, especially if the data has some order to it, you'd want to shuffle the data or use StratifiedKFold to maintain class distributions.\n",
                "\n",
                "---"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Single-Fold Validation vs. Cross-Validation for kNN Model Selection \n",
                "\n",
                "Now let's try both single-fold validation and cross-validation as techniques to choose the best `k` value for a k-Nearest Neighbors (KNN) regression model.\n",
                "\n",
                "## Single-Fold Validation\n",
                "\n",
                "Single-fold validation is a simple technique where we split our data into three parts: training, validation, and test sets. We use the training set to fit our model, the validation set to evaluate and select the best model, and the test set for final evaluation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# SINGLE-FOLD VALIDATION\n",
                "# Create a separate 75/25 split for single fold validation\n",
                "X_train_single, X_val_single, y_train_single, y_val_single = train_test_split(X_train, y_train, test_size=0.25, random_state=42, stratify=X_train.big_club_1)\n",
                "\n",
                "# Perform single fold validation\n",
                "single_fold_train_scores = []\n",
                "single_fold_val_scores = []\n",
                "\n",
                "k_values = range(1, 31) # Not to be confused with k-folds!!\n",
                "\n",
                "knn = KNeighborsRegressor()\n",
                "\n",
                "for k in k_values:\n",
                "    # No need to create a new knn object each time\n",
                "    knn.n_neighbors = k\n",
                "    knn.fit(X_train_single, y_train_single)\n",
                "    single_fold_train_scores.append(mean_squared_error(y_train_single, knn.predict(X_train_single)))\n",
                "    single_fold_val_scores.append(mean_squared_error(y_val_single, knn.predict(X_val_single)))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cross-Validation\n",
                "\n",
                "Cross-validation is more robust and makes better use of our data. Instead of using a single validation set, we divide our data into several folds and use each fold as a validation set once, while the remaining folds serve as the training set.\n",
                "\n",
                "In this example, we'll use 8-fold cross-validation, training and validating our model 8 times for each `k` value, each time using a different fold as the validation set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "## CROSS-VALIDATION\n",
                "cv_train_scores = []\n",
                "cv_val_scores = []\n",
                "\n",
                "# number of cv folds (to avoid confusion with knn's k)\n",
                "cv = 8\n",
                "\n",
                "for k in k_values:\n",
                "    knn.n_neighbors = k\n",
                "    \n",
                "    # Cross-validation scores\n",
                "    cv_results = cross_validate(knn, X_train, y_train, cv=cv, \n",
                "                                scoring='neg_mean_squared_error',\n",
                "                                return_train_score=True)\n",
                "    \n",
                "    cv_train_scores.append(-cv_results['train_score'])  # Convert to positive MSE\n",
                "    cv_val_scores.append(-cv_results['test_score'])    # Convert to positive MSE\n",
                "\n",
                "# Calculate mean and standard error of cross-validation scores\n",
                "train_means = np.mean(cv_train_scores, axis=1)\n",
                "train_ses = np.std(cv_train_scores, axis=1, ddof=1) / np.sqrt(cv)  # Standard error\n",
                "val_means = np.mean(cv_val_scores, axis=1)\n",
                "val_ses = np.std(cv_val_scores, axis=1, ddof=1) / np.sqrt(cv)  # Standard error"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualizing the Results\n",
                "\n",
                "Now that we've performed both single-fold validation and cross-validation, let's visualize the results. This will help us compare the two methods and understand their differences.\n",
                "\n",
                "We'll create two plots side by side:\n",
                "1. The cross-validation results, showing the mean training and validation MSE across all folds, with error bars representing one standard error.\n",
                "2. The single-fold validation results, showing the training and validation MSE for our single split.\n",
                "\n",
                "These plots will help us see how the model's performance changes with different `k` values and how the two validation methods compare:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot the results\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6), sharey=True)\n",
                "\n",
                "# Cross-validation plot\n",
                "ax1.plot(k_values, train_means, label=f'Train MSE (+/- 1 SE)')\n",
                "ax1.fill_between(k_values, train_means - train_ses, train_means + train_ses, alpha=0.1)\n",
                "ax1.plot(k_values, val_means, label=f'Mean Validation MSE (+/- 1 SE)')\n",
                "ax1.fill_between(k_values, val_means - val_ses, val_means + val_ses, alpha=0.1)\n",
                "ax1.set_xlabel('$k$')\n",
                "ax1.set_ylabel('Mean Squared Error')\n",
                "ax1.set_title('Cross-Validation')\n",
                "ax1.legend()\n",
                "ax1.grid(True)\n",
                "\n",
                "# Find the best k from cross-validation\n",
                "best_k_cv = k_values[np.argmin(val_means)]\n",
                "ax1.axvline(best_k_cv, color='r', linestyle='--', label=f'Best k = {best_k_cv}')\n",
                "ax1.legend()\n",
                "\n",
                "# Single fold plot\n",
                "ax2.plot(k_values, single_fold_train_scores, label='Train MSE')\n",
                "ax2.plot(k_values, single_fold_val_scores, label='Validation MSE')\n",
                "ax2.set_xlabel('$k$')\n",
                "ax2.set_ylabel('Mean Squared Error')\n",
                "ax2.set_title('Single Fold (75/25 split)')\n",
                "ax2.legend()\n",
                "ax2.grid(True)\n",
                "\n",
                "# Find the best k from single fold\n",
                "best_k_single = k_values[np.argmin(single_fold_val_scores)]\n",
                "ax2.axvline(best_k_single, color='r', linestyle='--', label=f'Best k = {best_k_single}')\n",
                "ax2.legend()\n",
                "\n",
                "plt.tight_layout()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Final Model Refitting and Evaluation\n",
                "\n",
                "Now that we've selected the best $k$ value using both methods, it's crucial to understand that our model selection process is **not yet complete!** Here's why:\n",
                "\n",
                "**Unused Information**: For the single-fold method, we've only used 75% of our training data to fit the model. For cross-validation, while we used all the data for validation, each fold's model was trained on only a subset of the data.\n",
                "\n",
                "So we need to take two important last steps:\n",
                "\n",
                "1. **Refit on All Training Data**: We'll train new models using the best $k$ values we found, but this time using all of our training data. This ensures we're using all available information to create our final models.\n",
                "\n",
                "2. **Evaluate on the Test Set**: We'll use our held-out test set, which hasn't been used in any part of our model selection process, to get an unbiased estimate of our models' performance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Refit models with best k values and evaluate using R-squared\n",
                "knn_cv = KNeighborsRegressor(n_neighbors=best_k_cv)\n",
                "knn_single = KNeighborsRegressor(n_neighbors=best_k_single)\n",
                "\n",
                "knn_cv.fit(X_train, y_train)\n",
                "knn_single.fit(X_train_single, y_train_single)\n",
                "\n",
                "# Evaluate on train and test sets using R-squared\n",
                "X_test = design_test_df.copy()\n",
                "train_r2_cv = r2_score(y_train, knn_cv.predict(X_train))\n",
                "test_r2_cv = r2_score(y_test, knn_cv.predict(X_test))\n",
                "\n",
                "train_r2_single = r2_score(y_train_single, knn_single.predict(X_train_single))\n",
                "test_r2_single = r2_score(y_test, knn_single.predict(X_test))\n",
                "\n",
                "print(\"\\nFinal Results:\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"{'Model':\u003c20} {'k':\u003e5} {'Train R²':\u003e10} {'Test R²':\u003e10}\")\n",
                "print(\"-\" * 50)\n",
                "print(f\"{'Cross-Validation':.\u003c20} {best_k_cv:\u003e5d} {train_r2_cv:\u003e10.4f} {test_r2_cv:\u003e10.4f}\")\n",
                "print(f\"{'Single Fold':.\u003c20} {best_k_single:\u003e5d} {train_r2_single:\u003e10.4f} {test_r2_single:\u003e10.4f}\")\n",
                "print(\"=\" * 50)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We've seen how single-fold validation and cross-validation can lead to different model selections (k=13 vs k=8). The cross-validation approach generally provides a more robust estimate of model performance, as it uses all of the training data for validation in different iterations.\n",
                "\n",
                "However, cross-validation is more computationally expensive, especially with large datasets or complex models. Single-fold validation can be a quicker alternative when computational resources are limited or when dealing with very large datasets."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv id=boot style=\"border: thin solid black; background: lightsalmon; padding: 5px\"\u003e\n",
                "\u003ch1\u003e2 - Bootstrapping\u003c/h1\u003e \n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cimg src=\"fig/bootstrap_example_rader.jpeg\" alt=\"alt text\" width=\"500\" height=\"300\" class=\"blog-image\"\u003e\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "### Key Points for Bootstrapping:\n",
                "\n",
                "1. **Sample Size Consistency:** \n",
                "    - **Key Takeaway:** The bootstrap dataset should have the same number of points as the original.\n",
                "    - **Rationale:** This preserves the structure and variability of the original dataset in the bootstrap samples, ensuring that we are drawing from a distribution similar to our original data.\n",
                "\n",
                "\u003cdiv style=\"padding-top:0px;\"\u003e\u003c/div\u003e\n",
                "\n",
                "2. **Sampling with Replacement:** \n",
                "    - **Key Takeaway:** We must sample with replacement; each time an item is selected, we put it back in the original dataset, so it may possibly be reselected.\n",
                "    - **Rationale:** By sampling with replacement, each bootstrap sample is an independent draw from the original dataset. This captures the idea that we are trying to emulate the process of obtaining new samples from the population.\n",
                "\n",
                "\u003cdiv style=\"padding-top:0px;\"\u003e\u003c/div\u003e\n",
                "\n",
                "3. **Extensive Iterative Sampling for Robust Bootstrapping Results:**\n",
                "    - **Key Takeaway:** Typically, thousands of bootstrap samples are drawn (e.g., 10,000 or more) to obtain a reliable estimate of the distribution.\n",
                "    - **Rationale:** The more replicates we draw, the better our approximation to the true distribution of the statistic. This ensures that our estimates, such as confidence intervals, are robust and reliable.\n",
                "\n",
                "\u003cdiv style=\"padding-top:0px;\"\u003e\u003c/div\u003e\n",
                "\n",
                "4. **Limitations and Assumptions of Bootstrapping:**\n",
                "    - **Key Takeaway:** While bootstrapping is versatile and powerful, it's not always the best approach for every situation.\n",
                "    - **Rationale:** The bootstrap method assumes that the sample is a good representation of the population. If the original sample has biases or is not representative, the bootstrap samples will inherit these issues. \n",
                "\n",
                "\u003cdiv style=\"padding-top:0px;\"\u003e\u003c/div\u003e\n",
                "\n",
                "5. **Bootstrap and Confidence Intervals:**\n",
                "    - **Key Takeaway:** One common use of bootstrapping is to construct confidence intervals, which provide a range of values that likely contain the true parameter value.\n",
                "    - **Rationale:** Bootstrapped confidence intervals give a non-parametric way to estimate the range of possible values for a statistic without making strong assumptions about the shape or parameters of the true distribution.\n",
                "\n",
                "---"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Bootstrap Implementation \n",
                "#### The laborious, but insightful way:\n",
                "\n",
                "1. Choose random data points by randomly choosing indices.\n",
                "2. Create subsets of the original data by choosing the DataFrame elements with the randomly chosen indices.\n",
                "\n",
                "This could be achieved by using numpy's `random.choice()` pandas' `.iloc[]` method."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create example df  \n",
                "example_X_df = X_train.head(5)\n",
                "example_y_df = y_train.head(5)\n",
                "\n",
                "# Generate random indices from example df\n",
                "boot_i = np.random.choice(example_X_df.index, replace = True, size = len(example_X_df.index)) \n",
                "\n",
                "# Generate an X_train_boot data frame that contains exactly the observations with the boot_i indices.\n",
                "# Print out the results of each line to understand what's going on. \n",
                "# Make sure that you understand why X_train_boot can contain the same index multiple times\n",
                "X_train_boot = example_X_df.iloc[boot_i]\n",
                "y_train_boot = example_y_df.iloc[boot_i]\n",
                "\n",
                "display(X_train_boot)\n",
                "display(y_train_boot)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### The fast way (if your training data wasn't split into X and y):\n",
                "\n",
                "1. Instead of calculating indices manually and using .iloc to grab the respective rows, you could also use pandas' built-in `.sample()` method\n",
                "2. However, since you don't have the indices to also grab the respective y_train_boot values in this case, you'd have to combine example_df and y_train"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create an example dataframe\n",
                "example_X_df = X_train.head(5)\n",
                "\n",
                "# Take the corresponding rows from y_train and add it to the example_df\n",
                "example_X_y_df = example_X_df.copy()\n",
                "example_X_y_df['target'] = y_train.head(5)\n",
                "\n",
                "# Bootstrapping using pandas' sample method\n",
                "boot_df = example_X_y_df.sample(frac=1, replace=True)\n",
                "\n",
                "# Split the bootstrapped dataframe back into X_train_boot and y_train_boot\n",
                "X_train_boot = boot_df.drop(columns='target')\n",
                "y_train_boot = boot_df['target']\n",
                "\n",
                "# display(X_train_boot)\n",
                "# display(y_train_boot)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class=\"alert alert-success\"\u003e\n",
                "\u003ch3\u003eYour turn! (10 mins)\u003c/h3\u003e \n",
                "\n",
                "Complete the code inside the loop below. Make sure to create a new bootstrap sample in every iteration, fit a linear regression model. Store the model in `boot_models` as well as the coefficients."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configure number of bootstraps\n",
                "n_boots = 1000\n",
                "\n",
                "# Lists to save models and coefficients\n",
                "boot_models = []\n",
                "boot_betas = []\n",
                "\n",
                "for i in range(n_boots):\n",
                "   \n",
                "    # Randomly sample indices with replacement to create bootstrap samples\n",
                "    # your code here\n",
                "    boot_i = ... \n",
                "\n",
                "    # Create bootstrap datasets for features and target variable using the sampled indices\n",
                "    X_train_boot = ...\n",
                "    y_train_boot = ...\n",
                "    \n",
                "    # Train a linear regression model on the bootstrap sample\n",
                "    boot_linreg = LinearRegression().fit(..., ...)\n",
                "\n",
                "    # Save the trained model\n",
                "    boot_models.append(...)\n",
                "    \n",
                "    # Extract and save coefficients from the trained model (including the intercept)\n",
                "    coefs = np.insert(boot_linreg.coef_[0], 0, boot_linreg.intercept_[0], axis=None)\n",
                "    boot_betas.append(coefs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "## load solutions if stuck\n",
                "# %load snips/boot.py"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Store betas in df with feature names\n",
                "feature_names_final = ['intercept'] + list(X_train.columns)\n",
                "boot_betas_df = pd.DataFrame(boot_betas, columns=feature_names_final)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class=\"alert alert-success\"\u003e\n",
                "\u003ch3\u003eNext Steps\u003c/h3\u003e \n",
                "\n",
                "if you have some time left, look into the following next steps. We will do this interactively in class.\n",
                "\n",
                "1. **Examine the Coefficients:** Take a closer look at the boot_betas to understand the distribution and variability of our bootstrapped coefficients.\n",
                "\n",
                "2. **Comparing Columns:** Determine the number of columns in boot_betas \u0026 the number of columns in X_train.\n",
                "\n",
                "\u003cdiv style='padding:0px'\u003e\u003c/div\u003e\n",
                "\n",
                "3. **Investigate Discrepancies:** Can you explain the difference in the number of columns between boot_betas and X_train?\n",
                "  \n",
                "\u003cdiv style='padding:0px'\u003e\u003c/div\u003e\n",
                "\n",
                "4. **Action Plan:** What steps do you think we need to take next, based on the results and discrepancies you've observed?"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Explore"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "boot_betas_df.describe()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv id=ci style=\"border: thin solid black; background: lightsalmon; padding: 5px\"\u003e\n",
                "\u003ch1\u003e3 - Bootstrap Confidence Intervals\u003c/h1\u003e \n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "After bootstrapping and obtaining a distribution of coefficients for each feature, a logical next step is to compute confidence intervals. Confidence intervals provide a range of values, derived from the bootstrapped data, in which we expect the true parameter (in this case, the coefficient) to lie, with a certain level of confidence.\n",
                "\n",
                "**Why is this useful?**\n",
                "\n",
                "- **Interpretability**: Confidence intervals help us understand the uncertainty and variability associated with our estimates. A narrower confidence interval indicates a more reliable estimate, while a wider interval suggests more uncertainty.\n",
                "  \n",
                "- **Statistical Significance**: By examining whether a confidence interval for a coefficient includes zero, we can make inferences about the statistical significance of that coefficient. If zero isn't in the interval, we might infer the corresponding feature has a significant association with the target.\n",
                "\n",
                "- **Comparisons**: By comparing the confidence intervals of different coefficients, we can gauge which features have more influence or certainty associated with their effects.\n",
                "\n",
                "In the following code, we will compute the 95% confidence intervals for each feature's coefficient. This means that we are 95% confident that the true coefficient lies within the provided range.\n",
                "\n",
                "---\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class=\"alert alert-success\"\u003e\n",
                "\u003ch3\u003eTASK: Find the 95% bootstrap confidence intervals for the coefficients\u003c/h3\u003e "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize an empty list to store statistics for each feature\n",
                "stats_list = []\n",
                "\n",
                "feature_names_final = X_train.columns\n",
                "\n",
                "# Loop through each feature to compute its bootstrapped statistics\n",
                "for i in range(len(feature_names_final)):\n",
                "    \n",
                "    # Extracting the bootstrapped coefficient values for the current feature\n",
                "    betavals = boot_betas_df.iloc[:, i]\n",
                "    \n",
                "    # Sorting the coefficient values to aid in percentile calculation\n",
                "    betavals.values.sort()\n",
                "    \n",
                "    # Calculating the 2.5th percentile - lower bound of the 95% CI\n",
                "    # your code here\n",
                "    x1 = np.round(np.percentile(betavals, ...), 2)\n",
                "    \n",
                "    # Calculating the 97.5th percentile - upper bound of the 95% CI\n",
                "    x2 = np.round(np.percentile(betavals, ...), 2)\n",
                "    \n",
                "    # Calculating mean and standard deviation of the bootstrapped coefficients\n",
                "    mean = np.round(np.mean(betavals),2)\n",
                "    std = np.round(np.std(betavals),2)\n",
                "    \n",
                "    # Appending computed statistics for current feature to the stats_list\n",
                "    stats_list.append([feature_names_final[i], mean, std, x1, x2])\n",
                "\n",
                "# Convert the stats_list into a dataframe for easy visualization and analysis\n",
                "boot_beta_df = pd.DataFrame(stats_list, columns=['feature', 'boot_mean', 'boot_std', '95_low', '95_high'])\n",
                "\n",
                "# Display the final dataframe with bootstrapped statistics and confidence intervals\n",
                "boot_beta_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "## load solutions if stuck\n",
                "# %load snips/boot-ci.py"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Now let's plot the confidence intervals for a couple of the feature coefficients"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setting up the figure and axes\n",
                "fig, axs = plt.subplots(8, 2, figsize=(10, 40))\n",
                "axs = axs.ravel()\n",
                "\n",
                "# Looping through each feature to plot histograms and confidence intervals\n",
                "# Starting from 1 to exclude the intercept (assuming x0 is the first column)\n",
                "for i, (index, row) in enumerate(boot_beta_df.iloc[1:].iterrows()):\n",
                "    sns.histplot(boot_betas_df.iloc[:, i+1], bins = 20, ax=axs[i], kde=True, color='skyblue', edgecolor='black')\n",
                "    axs[i].axvline(row['95_low'], color='red', linestyle='--', label='2.5% Percentile')\n",
                "    axs[i].axvline(row['95_high'], color='green', linestyle='--', label='97.5% Percentile')\n",
                "    axs[i].axvline(row['boot_mean'], color='blue', linestyle='-', label='Mean')\n",
                "    axs[i].set_title(row['feature'])\n",
                "    axs[i].legend()\n",
                "\n",
                "# Adjusting layout\n",
                "plt.tight_layout()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv id=feature style=\"border: thin solid black; background: lightsalmon; padding: 5px\"\u003e\n",
                "\u003ch1\u003e4 - Feature importance\u003c/h1\u003e "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's try and determine how 'important' our predictors are interms of their relationship to the response variable using the bootstrapped beta values in DataFrame `boot_beta_df`. First we sort the values."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "sorted_boot_beta_df = boot_beta_df.sort_values(by=['boot_mean'], ascending=True)\n",
                "sorted_boot_beta_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, ax = plt.subplots(figsize=(7, 4)) \n",
                "\n",
                "# Sort the dataframe by absolute value of boot_mean, excluding the intercept\n",
                "sorted_boot_beta_df = boot_beta_df.iloc[:-1].sort_values('boot_mean', key=abs, ascending=True)\n",
                "\n",
                "# Create horizontal bar plot\n",
                "bars = ax.barh(sorted_boot_beta_df['feature'], sorted_boot_beta_df['boot_mean'], \n",
                "               align='center', color=\"#336600\", alpha=0.7)\n",
                "\n",
                "# Add error bars\n",
                "ax.errorbar(sorted_boot_beta_df['boot_mean'], sorted_boot_beta_df['feature'],\n",
                "            xerr=sorted_boot_beta_df['boot_std'], fmt='none', ecolor='black', capsize=5, alpha=.4)\n",
                "\n",
                "# Customize the plot\n",
                "ax.grid(linewidth=0.2, axis='x')\n",
                "ax.set_xlabel('Coefficient Value')\n",
                "ax.set_ylabel('Predictors')\n",
                "ax.set_title('Bootstrap Coefficient Values')\n",
                "\n",
                "plt.tight_layout()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Now let's find the feature importance using the t-values \n",
                "\n",
                "(NOTE: The lambda function here takes a row of the DataFrame as input and returns the result of dividing 'boot_mean' by 'boot_std'. axis=1: This argument indicates that the function should be applied to rows, not columns."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# creating a new column with the t-values  \n",
                "sorted_boot_beta_df['t'] = sorted_boot_beta_df.apply(lambda row: \n",
                "                                         row['boot_mean']/row['boot_std'], axis=1)\n",
                "sorted_boot_beta_df = sorted_boot_beta_df.sort_values('t', key=abs)\n",
                "sorted_boot_beta_df.head(2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, ax = plt.subplots()\n",
                "ax.barh(sorted_boot_beta_df['feature'].iloc[:-1], sorted_boot_beta_df['t'].iloc[:-1], \n",
                "        align='center', color=\"#336600\", alpha=0.7)\n",
                "ax.grid(linewidth=0.2)\n",
                "ax.set_xlabel('Coefficient')\n",
                "ax.set_ylabel('Predictors')\n",
                "ax.set_title('Feature Importance based on $\\hat{t}-$test')\n",
                "plt.show()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv id=coeff_interpretation style=\"border: thin solid black; background: lightsalmon; padding: 5px\"\u003e\n",
                "\u003ch1\u003e5 - Linear Regression Coefficient Interpretation (with statsmodels)\u003c/h1\u003e "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Say we have input features $X$, which via some function $f()$, approximates outputs $Y$. That is, $Y = f(X) + \\epsilon$.\n",
                "\n",
                "- **Inference**: estimates the function $f$, and is more concerned with understanding the relationship between $X$ and $Y$.\n",
                "- **Prediction**: the goal is making accurate $Y$ predictions for some unseen $X$.\n",
                "\n",
                "\n",
                "#### Two popular and useful libraries in Python are `sklearn` and `statsmodels`.\n",
                "\n",
                "`statsmodels` is mostly focused on the _inference_ task. It aims to make good estimates for $f()$ (via solving for our $\\beta$'s), and it provides expansive details about its certainty. It provides lots of tools to discuss confidence, but isn't great at dealing with test sets.\n",
                "\n",
                "`sklearn` is mostly focused on the _prediction_ task. It aims to make a well-fit line to our input data $X$, so as to make good $Y$ predictions for some unseen inputs $X$. It provides a shallower analysis of our variables. In other words, `sklearn` is great at test sets and validations, but it can't really discuss uncertainty in the parameters or predictions."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Let's try `statsmodels` now.\n",
                "\n",
                "`statsmodels` linear regression does not include an intercept by default, so, if needed, we add one using:\n",
                "```\n",
                "X_train = sm.add_constant(X_train)\n",
                "sm.OLS(y,X_train)\n",
                "```\n",
                "where Xtrain is our train set and y our return variable. \n",
                "\n",
                "Watch for additional intercepts coming from, e.g. Polynomial features. Have only one intercept. In our case we do have one, so we are not adding another one. "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Let's load a clean set of data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train = pd.read_csv(\"data/design_train_df.csv\")\n",
                "y_train = pd.read_csv(\"data/y_train.csv\")\n",
                "\n",
                "X_test = pd.read_csv(\"data/design_test_df.csv\")\n",
                "y_test = pd.read_csv(\"data/y_test.csv\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Unlike sklearn's LinearRegression, statsmodels' OLS doesn't give you the column of ones 'for free' You need to add it to the X data using `sm.add_constant`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import statsmodels.api as sm\n",
                "from statsmodels.api import OLS\n",
                "\n",
                "# your code here\n",
                "X_train = sm.add_constant(...)\n",
                "ols = OLS(y_train, np.array(X_train))\n",
                "results = ols.fit()\n",
                "# get the parameters\n",
                "results.params "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "## load solutions if stuck\n",
                "# %load snips/ols.py"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Can we recover the feature names for readability?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "feature_names = list(X_train.columns)\n",
                "feature_names"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "results_df = pd.DataFrame(zip(feature_names, results.params), columns=['feature', 'coeff'])\n",
                "results_df"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "With `statsmodels` we can print more statistics than we need!\n",
                "```\n",
                "results.summary()\n",
                "```\n",
                "If we want human-readable feature names we need to set the parameter `xname`\n",
                "```\n",
                "results.summary(xname = a list of names)\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "results.summary(xname=feature_names)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class=\"alert alert-success\"\u003e\n",
                "    \u003cstrong\u003e 💬 DISCUSSION:\u003c/strong\u003e How do the bootstrap confidence intervals compare with the ones derived by `statsmodels` which are based on the Student’s t-distribution? \u003c/div\u003e "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Bootstrap confidence intervals and confidence intervals derived from the Student's \\( t \\)-distribution (as used in `statsmodels`) are two different methods to estimate the precision and uncertainty of parameter estimates in statistical models. Let's delve into both methods a bit:\n",
                "\n",
                "Bootstrap Confidence Intervals:\n",
                "1. Non-Parametric Approach:\n",
                "   - The bootstrap method does not assume that the data follows any particular distribution.\n",
                "2. Resampling Technique:\n",
                "   - It involves repeatedly resampling, with replacement, from the observed data, and then recalculating the estimate of interest on each resample.\n",
                "\n",
                "Confidence Intervals using Student’s \\( t \\)-Distribution (e.g., in `statsmodels`):\n",
                "1. Parametric Approach:\n",
                "   - Assumes the estimators of the model parameters are normally distributed or follow a Student’s \\( t \\)-distribution, especially in smaller sample sizes.\n",
                "2. Model-Based Standard Errors:\n",
                "   - Utilizes the standard errors calculated based on model assumptions (like independence of errors in OLS regression) to compute confidence intervals.\n",
                "3. Analytical Solution:\n",
                "   - It uses an analytical solution based on the underlying statistical theory, making it computationally less intensive than bootstrap.\n",
                "Comparison:\n",
                "Bootstrap can be applied to a wider range of statistics, including those for which there is no straightforward analytical method for calculating confidence intervals. The \\( t \\)-distribution method might be limited to parameters for which there are well-established methods for calculating standard errors under model assumptions. Also, Bootstrap might be preferred for deriving confidence intervals for complex statistics or in the absence of theoretical distributions. \\( t \\)-distribution intervals might be preferred for simplicity and speed when model assumptions are valid."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv class=\"alert alert-success\"\u003e\n",
                "    \u003cstrong\u003e 💬 DISCUSSION:\u003c/strong\u003e What conclusions can we make from the values of the model coefficients as to the relationship between our response variable y and the features? \u003c/div\u003e \n",
                "\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003cdiv id=model style=\"border: thin solid black; background: lightsalmon; padding: 5px\"\u003e\n",
                "\u003ch1\u003e6 - Model Selection for Ridge and LASSO (example code).\u003c/h1\u003e "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We are interested in the generalization performance of a learning method which shows as the prediction capability on an independent test data. This performance guides our choice of learning model, and gives us a measure of the quality of the ultimately chosen model. If we are in a data-rich situation, the best approach for both problems is to randomly divide the dataset into three parts: a training set, a validation set, and a test set. The training set is used to fit the models; the validation set is used to estimate prediction error for model selection; the test set is used for assessment of the generalization error of the final chosen model.\n",
                "\n",
                "- **Model selection**: estimating the performance of different models in order to choose the best one.\n",
                "- **Model assessment**: having chosen a final model, estimating its predic- tion error (generalization error) on new data.\n",
                "\u003cp style=\"text-align:right\"\u003e\u003cfont size=\"1\"; text-align='right'\u003e(The Elements of Statistical Learning (2009)\u003c/font\u003e\u003c/p\u003e"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Ridge regression using a validation set. "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Let's load a clean set of data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train = pd.read_csv(\"data/design_train_df.csv\")\n",
                "y_train = pd.read_csv(\"data/y_train.csv\")\n",
                "\n",
                "X_test = pd.read_csv(\"data/design_test_df.csv\")\n",
                "y_test = pd.read_csv(\"data/y_test.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train.head(2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split the data into train and validation sets \n",
                "X_train, X_val, y_train, y_val = train_test_split(X_train, \n",
                "                                                  y_train, \n",
                "                                                  train_size=0.8, \n",
                "                                                  random_state=42\n",
                "                                                  )\n",
                "\n",
                "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# List of hyper-parameter values \n",
                "alphas = np.logspace(0, 2.5, 100)\n",
                "\n",
                "# List to store training errors\n",
                "training_error = []\n",
                "# List to store validation errors\n",
                "validation_error = []\n",
                "\n",
                "fit_intercept = True\n",
                "\n",
                "for i, alpha in enumerate(alphas):\n",
                "    \n",
                "    # For each i, fit a ridge regression on training set\n",
                "    ridge_reg = Ridge(fit_intercept=fit_intercept, \n",
                "                      alpha=alpha,\n",
                "                      #max_iter=max_iter\n",
                "                     )\n",
                "    ridge_reg.fit(X_train, y_train)\n",
                "\n",
                "    # Predict on the train and validation set \n",
                "    y_train_pred = ridge_reg.predict(X_train)\n",
                "    y_val_pred = ridge_reg.predict(X_val)\n",
                "    \n",
                "    # Compute the training and validation MSE\n",
                "    mse_train = mean_squared_error(y_train, y_train_pred) \n",
                "    mse_val = mean_squared_error(y_val, y_val_pred)\n",
                "    \n",
                "    # Add that value to the list \n",
                "    training_error.append(mse_train) \n",
                "    validation_error.append(mse_val) "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# find the best parameter\n",
                "best_alpha = alphas[validation_error.index(min(validation_error))]\n",
                "print(\"Ridge Best Alpha:\", best_alpha)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, ax = plt.subplots(figsize=(6,4))\n",
                "\n",
                "ax.plot(alphas, training_error, label='Training Error');\n",
                "ax.plot(alphas, validation_error, label='Validation Error');\n",
                "ax.axvline(best_alpha, c='k', ls=':', label=f\"Best Alpha: {best_alpha:.2f}\")\n",
                "\n",
                "ax.set_xlabel('Ridge tuning parameter (alpha)', fontsize=10)\n",
                "ax.set_ylabel('mse', fontsize=10)\n",
                "\n",
                "ax.set_title('Ridge Regression mse in train and test set', fontsize=12)\n",
                "ax.grid(\":\", alpha=0.4)\n",
                "\n",
                "ax.legend(loc='best')\n",
                "plt.tight_layout()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Let's re-fit the model on the whole dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# We should use all the dataset now, training+validation\n",
                "final_ridge_model = Ridge(alpha=best_alpha).fit(X_train, y_train)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Let's do the final assessment on the test set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_test_pred = final_ridge_model.predict(X_test)\n",
                "r2_test = final_ridge_model.score(X_test, y_test)\n",
                "print(f'R^2 test = {r2_test:.3}')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Lasso regression using cross validation. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set parameters for cross-validation\n",
                "alphas = np.logspace(-3, 0, 100)\n",
                "fit_intercept = True\n",
                "# Maximum number of iterations to run \n",
                "# before it converges.\n",
                "max_iter = 1000\n",
                "\n",
                "lasso_coefficients = []\n",
                "k = 5\n",
                "num_est = 0\n",
                "# store MSE results\n",
                "results = []\n",
                "\n",
                "training_error = []\n",
                "validation_error = []\n",
                "validation_std = []\n",
                "\n",
                "for alpha in alphas:\n",
                "\n",
                "    lasso_reg = Lasso(\n",
                "                fit_intercept=fit_intercept, \n",
                "                alpha=alpha, \n",
                "                #max_iter=max_iter\n",
                "    )\n",
                "    \n",
                "    lasso_cv = cross_validate(\n",
                "                lasso_reg,\n",
                "                X_train, \n",
                "                y_train, \n",
                "                cv=k, \n",
                "                scoring=\"neg_mean_squared_error\",\n",
                "                return_train_score=True,\n",
                "                return_estimator=True\n",
                "    )\n",
                "    \n",
                "    training_error.append(                       \n",
                "        np.mean(-lasso_cv['train_score'])\n",
                "    )\n",
                "    \n",
                "    validation_error.append(\n",
                "        np.mean(-lasso_cv['test_score'])\n",
                "    )\n",
                "    \n",
                "    validation_std.append(\n",
                "        np.std(lasso_cv['test_score'])\n",
                "    )\n",
                "    \n",
                "    lasso_coefficients.append(\n",
                "         lasso_cv['estimator']\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# find the best parameter\n",
                "min_cross_val_mse = min(validation_error)\n",
                "best_cross_val_alpha = alphas[validation_error.index(min_cross_val_mse)]\n",
                "min_cross_val_mse, best_cross_val_alpha"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, ax = plt.subplots(figsize=(6,4))\n",
                "\n",
                "ax.plot(alphas, training_error, label='training error');\n",
                "ax.plot(alphas, validation_error, label='validation error');\n",
                "ax.axvline(best_cross_val_alpha, c='k', ls=':', label=f\"Best Alpha: {best_cross_val_alpha:.2f}\")\n",
                "\n",
                "\n",
                "ax.set_xlabel('Lasso tuning parameter (alpha)', fontsize=10)\n",
                "ax.set_ylabel('mse', fontsize=10)\n",
                "\n",
                "ax.set_title('Lasso Regression mse in train and validation set', fontsize=12)\n",
                "ax.grid(\":\", alpha=0.4)\n",
                "\n",
                "ax.legend(loc='best')\n",
                "ax.set_xscale('log')\n",
                "plt.tight_layout()"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Let's re-fit the model on the whole dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# We should use all the dataset now, training+validation\n",
                "cross_lasso_model = Lasso(alpha=best_cross_val_alpha).fit(X_train, y_train)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Evaluate on the test set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "y_test_pred = cross_lasso_model.predict(X_test)\n",
                "r2_test = cross_lasso_model.score(X_test, y_test)\n",
                "print(f'R^2 test = {r2_test:.3}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Notice the zeroed coefs from LASSO\n",
                "cross_lasso_model.coef_"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Ridge and lasso regression using built-in cross validation in `sklearn`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_train = pd.read_csv(\"data/design_train_df.csv\")\n",
                "y_train = pd.read_csv(\"data/y_train.csv\")\n",
                "\n",
                "X_test = pd.read_csv(\"data/design_test_df.csv\")\n",
                "y_test = pd.read_csv(\"data/y_test.csv\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "X_train.head(2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set parameters for cross-validation\n",
                "ridge_alphas = np.logspace(0, 2.75, 500)\n",
                "lasso_alphas = np.logspace(-3, 0, 500)\n",
                "\n",
                "# These CV models refit on the full training data\n",
                "# using the best alpha found during cross-validation!\n",
                "ridge = RidgeCV(alphas=ridge_alphas, cv=k).fit(X_train, y_train)\n",
                "lasso = LassoCV(alphas=lasso_alphas, cv=k).fit(X_train, y_train)\n",
                "\n",
                "# Best alphas \u0026 test scores\n",
                "ridge_a = ridge.alpha_\n",
                "print('Best alpha for ridge: {}'.format(ridge_a))\n",
                "print(f'R^2 score for Ridge with alpha={ridge_a}: {ridge.score(X_test, y_test):.3}')\n",
                "\n",
                "lasso_a = lasso.alpha_\n",
                "print('Best alpha for lasso: {}'.format(lasso_a))\n",
                "print(f'R^2 score for Lasso with alpha={lasso_a}: {lasso.score(X_test, y_test):.3}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                ""
            ]
        }
    ]
}
