{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17d30ffd-a184-4038-8e6f-47bd7468274a",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109A Introduction to Data Science \n",
    "\n",
    "# Lab 10: Bagging, Random Forests, & Interpreting ML Models\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Fall 2024**<br/>\n",
    "**Instructors**: Pavlos Protopapas and Natesh Pillai<br/>\n",
    "<hr style='height:2px'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ae354d-15ec-4029-828a-536d0f76773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd626d5-3411-481e-a16a-373e372c6e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress pesky warnings (probably bad practice)\n",
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64c84fe9-9163-464f-aee3-cbbdac2ed317",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "- Cambridge House Prices Regression Dataset: EDA\n",
    "- Single Tuned Tree\n",
    "- Bagging Ensemble\n",
    "- Out-of-Bag Error (OOB)\n",
    "- Random Forest\n",
    "- Model Interpretation\n",
    "    - Interpretation Through Predictions\n",
    "    - Feature Importance \n",
    "    - Permutation Importance\n",
    "    - LIME "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24e8be35-aa60-4e73-9856-cf668ae93564",
   "metadata": {},
   "source": [
    "## Regression Dataset: Cambridge housing prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35eb499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "homes = pd.read_csv(\"data/cambridgehomes.csv\")\n",
    "homes_raw = homes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fe59ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bare minimum preprocessing\n",
    "homes[\"zip\"] = homes[\"zip\"].astype(str)\n",
    "homes['lotsize'] = homes['lotsize'].fillna(0)\n",
    "homes['hoa'] = homes['hoa'].fillna(0)\n",
    "type_dummies = pd.get_dummies(homes['type'], drop_first=True)\n",
    "homes = pd.concat([homes,type_dummies],axis=1)\n",
    "homes.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5ebcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log2(homes['price']/1000)\n",
    "X = homes[['multifamily', 'singlefamily', 'townhouse',\n",
    "                'sqft',  'dist', 'beds', 'baths', 'year','hoa','lotsize']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a514d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa9b72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.plot(kind='hist', title=r\"$\\log_2$(price)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f10024",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrow = 2; ncol = 5;\n",
    "fig, axs = plt.subplots(nrows=nrow, ncols=ncol, figsize=(20, 8))\n",
    "\n",
    "fig.suptitle('EDA for Home dataset', fontsize=16)\n",
    "\n",
    "for ax, column in zip(axs.reshape(-1), X_train.columns):\n",
    "    ax.scatter(X_train[column], y_train, alpha=0.2)\n",
    "    ax.set_xlabel(f'{column}', fontsize=14)\n",
    "    ax.set_ylabel('Home Price', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbe3bc3-607a-4702-8dae-1d63b5526a14",
   "metadata": {},
   "source": [
    "## Tuned Decision Tree\n",
    "\n",
    "Our first model is a single decision tree. We use CV to select the best value for the `max_leaf_nodes` stopping criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4936dc6d-f2ba-499f-a478-2b72d4198b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeRegressor()\n",
    "params = {'max_leaf_nodes': [10, 15, 20, 25, 30]}\n",
    "# grid = GridSearchCV(dtree, params, cv=5, scoring='neg_mean_squared_error')\n",
    "grid = GridSearchCV(dtree, params, cv=5, scoring='r2')\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80057f51-c2d1-4ac7-ab0a-e077da20503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tree = grid.best_estimator_\n",
    "best_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8134b32e-ca88-44b8-904c-8d8399a5f323",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "tree.plot_tree(best_tree, feature_names=X_train.columns);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c463ed1d-3494-4d42-a868-d88856dbd377",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_train_r2 = best_tree.score(X_train, y_train)\n",
    "dt_cv_r2 = grid.cv_results_['mean_test_score'].max()\n",
    "dt_test_r2 = best_tree.score(X_test, y_test)\n",
    "print(f\"Train R^2:\\t{dt_train_r2:.4f}\")\n",
    "print(f\"CV R^2:\\t\\t{dt_cv_r2:.4f}\")\n",
    "print(f\"Test R^2:\\t{dt_test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3238f43-f93b-4412-b559-4db7f3282380",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6bc69d63-b62d-4778-a64e-e6e8dfc9aa9c",
   "metadata": {},
   "source": [
    "<img src=\"fig/bagging1.png\" alt=\"bagging\" width=\"60%\"/>\n",
    "\n",
    "In this section  we'll\n",
    "\n",
    "    1. Fit a Bagging classifer using multiple bootstrapped datasets and do majority voting. \n",
    "    2. Evaluate the model on OOB and feature importance.\n",
    "    \n",
    "Hopefully after this lab you will be able to answer the following questions: \n",
    "\n",
    "- What is the main idea behind bagging?\n",
    "- Why does bagging help with overfitting?\n",
    "- Why does bagging help to built more expressive trees?\n",
    "- What is OOB? How should we use it?\n",
    "- How can we measure feature importance with trees?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac2dacac-986d-4546-93cd-e5bc38de9ac4",
   "metadata": {},
   "source": [
    "\n",
    "*QUESTION:* Where does the word *\"Bagging\"* come from?\n",
    "\n",
    "**Some Theory: What is bagging?**\n",
    "  1. Bootstrapping: resample with replacements to get different datasets and built different models.\n",
    "  2. Do something smart to combine the different models.\n",
    "  \n",
    "One way to adjust for the high variance of the output of an experiment is to perform the experiment multiple times and then average the results. \n",
    "\n",
    " 1. **Bootstrap:** we generate multiple samples of training data, via bootstrapping. We train a full decision tree on each sample of data. \n",
    " 2. **AGGregatiING** for a given input, we output the averaged outputs of all the models for that input. \n",
    " \n",
    "This method is called **Bagging: B** ootstrap + **AGG**regat**ING**. \n",
    "\n",
    "-----------\n",
    "\n",
    "Let's bootstrap our training dataset to create multiple datasets and fit Decision Tree models to each.\n",
    "\n",
    "(Resampling: we showed live that different samples give different results for things like sums, varying more when the things we sum over have high variance themselves.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e29821-372e-44ed-a534-ce228b47ed75",
   "metadata": {},
   "source": [
    "### Bagging from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7384a5-dcb0-42de-aec4-a89845f230df",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = 1000 # This can be tuned\n",
    "# Create a decision tree to serve as the base model in the bagged ensemble\n",
    "best_params = best_tree.get_params()\n",
    "# model = DecisionTreeRegressor().set_params(**best_params)\n",
    "# model = DecisionTreeRegressor(max_leaf_nodes=int(2*best_params['max_leaf_nodes']))\n",
    "model = DecisionTreeRegressor(max_depth=None)\n",
    "\n",
    "# Initializing arrays to store results\n",
    "predictions_train = np.zeros( shape = (X_train.shape[0], n_trees))\n",
    "predictions_test  = np.zeros( shape = (X_test.shape[0],  n_trees))\n",
    "\n",
    "# Conduct bootstraping iterations\n",
    "for i in range(n_trees):\n",
    "    boot_idx = np.random.choice(X_train.shape[0], size=X_train.shape[0], replace=True)\n",
    "    boot_X = X_train.iloc[boot_idx]\n",
    "    boot_y = y_train.iloc[boot_idx]\n",
    "\n",
    "    # Fir on bootstrap\n",
    "    model.fit(boot_X, boot_y)  \n",
    "\n",
    "    # Store train and test predictions\n",
    "    predictions_train[:,i] = model.predict(X_train)   \n",
    "    predictions_test[:,i] = model.predict(X_test)\n",
    "    \n",
    "# Make Predictions Dataframe\n",
    "columns = [\"Bootstrap-Model_\"+str(i+1) for i in range(n_trees)]\n",
    "predictions_train = pd.DataFrame(predictions_train, columns=columns)\n",
    "predictions_test = pd.DataFrame(predictions_test, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32952e7-b36f-4273-b98d-5e8ad9a52873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregated predictions evaluation\n",
    "bag_train_r2 = r2_score(y_train, predictions_train.mean(axis=1))\n",
    "bag_test_r2 = r2_score(y_test, predictions_test.mean(axis=1))\n",
    "print(f\"Train R^2:\\t{bag_train_r2:.4f}\")\n",
    "print(f\"Test R^2:\\t{bag_test_r2:.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5769e7c1-87ba-412c-9c1a-dc427e6eac48",
   "metadata": {},
   "source": [
    "\n",
    "**Q:** ü§î Are these bagging models independent of each other, can they be trained in a parallel fashion?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ce4292-6b69-4463-a701-b5c16f98745c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468030e2-eb53-4fd4-936b-ea5f8889fabe",
   "metadata": {},
   "source": [
    "### Out-of-Bag Score (OOB)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "088ff063-2a76-4e73-b705-dd709325a5da",
   "metadata": {},
   "source": [
    "*QUESTION:* \n",
    "- What is out-of-bag (OOB) error? \n",
    "- How can we take advantage if it to improve our model's performance?\n",
    "- Why OOB is a great method?\n",
    "\n",
    "<img src=\"fig/oob.png\" alt=\"tree_adj\" width=\"60%\"/>\n",
    "\n",
    "Out-of-bag (OOB) error/Out-of-bag estimate is validated method availble to us whenever we use a bagged ensemble.\n",
    "OOB samples can be seen as the validation set, generated by bootstrap process. So, we don‚Äôt need to do the explicit setup another validation set. Compared to CV, it has the following advantages.\n",
    "\n",
    "There is less computational cost for OOB error compared to traditional CV. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aaa4f3-6b52-423e-91c0-898252fac1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = 1000 # This can be tuned\n",
    "# Create a decision tree to serve as the base model in the bagged ensemble\n",
    "best_params = best_tree.get_params()\n",
    "# model = DecisionTreeRegressor().set_params(**best_params)\n",
    "# model = DecisionTreeRegressor(max_leaf_nodes=int(2*best_params['max_leaf_nodes']))\n",
    "model = DecisionTreeRegressor(max_depth=None)\n",
    "\n",
    "# Initializing arrays to store results\n",
    "predictions_train = np.zeros( shape = (X_train.shape[0], n_trees))\n",
    "predictions_test  = np.zeros( shape = (X_test.shape[0],  n_trees))\n",
    "predictions_oob = np.zeros(shape = (X_train.shape[0], n_trees))*pd.NA # fill /w NA by default\n",
    "\n",
    "# Conduct bootstraping iterations\n",
    "for i in range(n_trees):\n",
    "    boot_idx = np.random.choice(X_train.shape[0], size=X_train.shape[0], replace=True)\n",
    "    boot_X = X_train.iloc[boot_idx]\n",
    "    boot_y = y_train.iloc[boot_idx]\n",
    "    \n",
    "    model.fit(boot_X, boot_y)  \n",
    "    predictions_train[:,i] = model.predict(X_train)   \n",
    "    predictions_test[:,i] = model.predict(X_test)\n",
    "    \n",
    "    # Get OOB samples\n",
    "    oob_mask = ~np.isin(np.arange(X_train.shape[0]), boot_idx)\n",
    "    X_oob = X_train[oob_mask]\n",
    "    y_oob = y_train[oob_mask]\n",
    "    oob_p = model.predict(X_oob)\n",
    "    predictions_oob[oob_mask, i] = oob_p # Update prediction results of this tree\n",
    "    \n",
    "# Make Predictions Dataframe\n",
    "columns = [\"Bootstrap-Model_\"+str(i+1) for i in range(n_trees)]\n",
    "predictions_train = pd.DataFrame(predictions_train, columns=columns)\n",
    "predictions_oob = pd.DataFrame(predictions_oob, columns=columns)\n",
    "predictions_test = pd.DataFrame(predictions_test, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77abc03a-06b4-4048-a3b3-d6665e3a8942",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_train_r2 = r2_score(y_train, predictions_train.mean(axis=1))\n",
    "bag_test_r2 = r2_score(y_test, predictions_test.mean(axis=1))\n",
    "bag_oob_r2 = r2_score(y_train, predictions_oob.mean(axis=1))\n",
    "print(f\"Train R^2:\\t{bag_train_r2:.4f}\")\n",
    "print(f\"OOB R^2:\\t{bag_oob_r2:.4f}\")\n",
    "print(f\"Test R^2:\\t{bag_test_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9dbad0-b786-4525-9051-aae8ed906552",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e17c2151-5ec6-4fb0-bfe6-5712399e0dcf",
   "metadata": {},
   "source": [
    "### SKLearn's BaggingRegressor\n",
    "\n",
    "From SKlearn's [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html):\n",
    "\n",
    "\"A Bagging regressor is an ensemble meta-estimator that fits base regressors each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.\"\n",
    "\n",
    "The most important arguments are `estimator` which defines the base estimators from which the bagged ensemble is composed and well as `n_estimators`, the number of estimators in the ensemble.\n",
    "If you want access to the `oob_score_` attribute on the fitted BaggingRegressor you will also need to set `oob_score=True` since it is `False` by default to save a bit of computation.\n",
    "\n",
    "The individual estimators in the ensemble can be accessed by the `estimators_` attribute.\n",
    "\n",
    "`warm_start=True` will cause the BaggingRegressor to re-use the previous estimators when re-fitting with a larger setting of `n_estimators`. This saves computation as you only need to fit the new trees being added to the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14a5a93-fbc8-4eca-b756-a65c13405193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a BaggingRegressor using a DecisionTreeRegressor as its base estimator\n",
    "bag = BaggingRegressor(estimator=DecisionTreeRegressor(),\n",
    "                  n_estimators=100,\n",
    "                  oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea250f4b-7791-496d-abd1-7d85a72b1e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on train\n",
    "bag.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7cea9d-3856-4757-9901-f98c876ee745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect OOB accuracy\n",
    "print(\"OOB ACC:\", bag.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62afd57d-2c95-4007-911f-749440b89875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect OOB accuracy\n",
    "print(\"OOB ACC:\", bag.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0837187-75d6-4730-be52-4f46cdda4727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect test accuracy\n",
    "print(\"Test ACC:\", bag.score(X_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09d90e88-03a6-49e3-becf-84c4da449a41",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <strong>üèãüèª‚Äç‚ôÇÔ∏è ACTIVITY:</strong> Number of Estimator's Affect on BaggingRegressor </div>  \n",
    "\n",
    "- Create a visualization showing how the number of estimators effects the bagging regressor's train and OOB scores.\n",
    "- You should use the default, 'full-depth' trees as your base estimators.\n",
    "- There are many ways to accomplish this!\n",
    "\n",
    "**Hint:** You may want to adjust the y-axis limits of your plot. Why is this necessary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3ecaf6-a8fe-4ec0-86b7-a07cfd30e8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6dc7c7bd-0055-442f-9991-4df0dc0e9c2d",
   "metadata": {},
   "source": [
    "**Q**: ü§î Can you think of a way to further reduce the variance of the ensemble model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fb6aaf-e804-4864-84bb-7c996580f4ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Weaknesses of Bagging\n",
    "\n",
    "Bagging is a *greedy* algorithm. What does this mean?\\\n",
    "We always choose to split on the feature with the most impact: i.e., the most information gain.\n",
    "\n",
    "$\\bullet$ Because of their greedy nature, bagging ensembles are very likely to be correlated, especially in the shallower nodes of the individual decision trees.\n",
    "\n",
    "### Why are decision trees greedy?\n",
    "\n",
    "$\\bullet$ Finding the optimal decision trees is an NP-complete problem, so in practice this is infeasible.\n",
    "\n",
    "$\\bullet$ Thus decision trees are **hueristic algorithms**. Hueristic algorithms are designed to solve problems by sacrificing optimality, accuracy, or precision in favor of speed. Hueristic algorithms are often used to solve NP-complete problems.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "264ab689-aa45-468d-9790-5c1b3ba5cd88",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5b789d1-c440-4870-ae1f-9dadb626d93c",
   "metadata": {},
   "source": [
    "Random Forest is a modified form of bagging that creates ensembles of independent decision trees. \n",
    "To *de-correlate the trees*, we: \n",
    "1. train each tree on a new bootstrapped sample of the full training set (same as in bagging) \n",
    "2. for each tree, at each split, we **randomly select a set of ùêΩ‚Ä≤ predictors from the full set of predictors.** (not done in bagging)\n",
    "3. From among the ùêΩ‚Ä≤  predictors, we select the optimal predictor and the optimal corresponding threshold for the split. \n",
    "\n",
    "<font color='red'> *Question:* </font>  Why would this second step help (only considering random sub-group of features)?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff87352c-7169-47e3-9d51-8e02c1083897",
   "metadata": {},
   "source": [
    "The averaging in Random Forest is able to reduce even more variance because the resulting trees are less correlated.\\\n",
    "This reduces overfitting as seen in the smaller gap between the train and OOB scores."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfd9d585-b09a-4c12-b45d-1bb51c43a6a3",
   "metadata": {},
   "source": [
    "Fit a RandomForestRegressor using the same `max_depth` and `n_estimators` as your final, large bagging model. But here we will set the `max_features` parameter to 'sqrt' to only consider a random set of predictors of size $\\sqrt{J}$ at each split, where $J$ is the original number of predictors.\n",
    "\n",
    "<font color='red'> NOTE: </font>  Please referring to the SKlearn documentation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff8fc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(max_depth=None,\n",
    "                           n_estimators=1000,\n",
    "                           max_features='sqrt',\n",
    "                           oob_score=True,\n",
    "                           random_state=109)\n",
    "rf.fit(X_train, y_train)\n",
    "print(f'The train score is: {rf.score(X_train, y_train):.4f}')\n",
    "print(f'The test score is: {rf.score(X_test, y_test):.4f}')\n",
    "print(f'The OOB score is: {rf.oob_score_:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbd97bbd-deed-4008-8e5f-329abd485878",
   "metadata": {},
   "source": [
    "### Tuning a Random Forest\n",
    "\n",
    "**Which parameters need to be tuned for a random forest model?**. Hint: there are 2 (or maybe 3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "219c593d-610d-4474-b725-765cdcd2010b",
   "metadata": {},
   "source": [
    "Use Cross-Validation (via `GridSearchCV`) or out-of-bag error (similar to the bagged model above) to tune a random forest model.  Consider 'min_samples_split' in the set of [2,5,10,20] and 'max_features' in the set of [2,5,8].\n",
    "\n",
    "<font color='red'> NOTE: </font> The choices of parameters under param_grid is based on the attributes in your estimator. For example, if your estimator is RandomForestRegressor, the choices of parameters are the attributes of the RandomForestRegressor, and you should refer to the SKlearn RandomForestRegressor documentation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7bd816",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2 = GridSearchCV(estimator=RandomForestRegressor(random_state=109),\n",
    "                  param_grid={'min_samples_split': [2,5,10,20],\n",
    "                              'max_features': [2,5,8]},\n",
    "                  cv=5)\n",
    "rf2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece37a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just refitting outside of GridSearchCV so that we can extract Variable Importances later\n",
    "rf_tuned = RandomForestRegressor(max_features=rf2.best_params_['max_features'],\n",
    "                                 min_samples_split=rf2.best_params_['min_samples_split'],\n",
    "                                 random_state=109).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd4b877",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2.best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "848e9eef-9bbe-4f97-85d3-c5ed024688f5",
   "metadata": {},
   "source": [
    "**Q**: ü§î A bagging model is just a special case of a random forest model.  Discuss.\n",
    "\n",
    "**Q**: ü§î **Why** do random forests improve predictions over a bagging model?  Under what conditions (think relationships among all of your variables) will this improvement be large?  When will it be small or perform even worse?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9734701d-afd4-4215-b6f4-687f1640d3a1",
   "metadata": {},
   "source": [
    "**Q**: ü§î A bagging model is just a special case of a random forest model.  Discuss.\n",
    "\n",
    "**Q**: ü§î **Why** do random forests improve predictions over a bagging model?  Under what conditions (think relationships among all of your variables) will this improvement be large?  When will it be small or perform even worse?\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a867078e-80b4-49ca-88c3-04c59b875825",
   "metadata": {},
   "source": [
    "## Model Interpretability üîé\n",
    "\n",
    "- As a data scientist, you need to know ins and outs of machine learning models.\n",
    "- Often you need to provide a **trustworthy, transparent, and accountable explanation** to stakeholders beyond the final evaluation metrics (e.g., $R^2$, MSE, Accuracy, ROC-AUC score, etc.).\n",
    "- To provide a good justification, we would do well to have an easily generalizable framework to explain a machine learning model.\n",
    "\n",
    "But there are inherent trade-off between model interpretability and accuracy.\\\n",
    "And there are many ways to explain the machine learning models:\n",
    "- Use only interpretable models\n",
    "    - Linear regression, logistic regression, decision trees, etc.\n",
    "    - But these are often times too simple and don't perform well!\n",
    "- Use model specific interpretation methods\n",
    "    - Lacks generalization because it is model dependent!\n",
    "- Use **model-agnostic methods**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b15558fd-6d1d-488a-8013-b06e24686e58",
   "metadata": {},
   "source": [
    "#### Fitting 5 tree-based models to predict `log2(price)` from `sqft` alone"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2e0eced-5b59-4e8d-be8c-1ec9d7efc091",
   "metadata": {},
   "source": [
    "The simplest (and best?) way to interpret a machine learning model is through the plotting the predictions! For a regression-type problem, that means visualizing $\\hat{y}$ vs. $x_j$.  For a calssification problem, that means visualizing $\\hat{P}(y=1)$ vs. $x_j$ or visualizing the decision boundary vs. $x_j$ and $x_{j`}$.\n",
    "\n",
    "When there is just a single predictor in the model, then plotting the predictions is easy (see below)!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4494d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Range of observed sqft predictor\n",
    "dummy_x = np.arange(np.min(homes['sqft']), np.max(homes['sqft']),1)\n",
    "\n",
    "plt.plot(X_train['sqft'], y_train,'.',alpha=0.5)\n",
    "\n",
    "bag_simple = RandomForestRegressor(min_samples_split=50,\n",
    "                           n_estimators=500,\n",
    "                           max_features=1,\n",
    "                           oob_score=True,\n",
    "                           random_state=109)\n",
    "bag_simple.fit(X_train[['sqft']], y_train)\n",
    "\n",
    "plt.plot(dummy_x, bag_simple.predict(dummy_x.reshape(-1,1)), \n",
    "             label=\"bag_simple\", alpha=1, lw=2)\n",
    "\n",
    "for i in [5, 3, 2, 1]:\n",
    "    dtree = tree.DecisionTreeRegressor(max_depth=i)\n",
    "    dtree.fit(X_train[['sqft']],y_train)\n",
    "    plt.plot(dummy_x, dtree.predict(dummy_x.reshape(-1,1)), \n",
    "             label=(\"max depth = \"+str(i)), alpha=0.7, lw=2)\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "938b7492-6ec8-4618-bdf2-33c8ed739d6c",
   "metadata": {},
   "source": [
    "**Q**: ü§î Which of these models are overfit?  Which are potential underfit?  What do the models say about the relationship between `log2(price)` and `sqft`?  Why isn't there technically a random forest model?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "589b0120-b912-4eb8-98e7-926c5e752f72",
   "metadata": {},
   "source": [
    "#### Fitting Four multiple-predictor ML Models\n",
    "\n",
    "When there are multiple predictors, things get tough! We fit 4 different tree-based models using the 10 predictor $X$ from before:\n",
    "\n",
    "Start with 2 decision tree to add to our previous ensemble models: the tuned random forest model (`rf2`) and the untuned bagged model (`bag`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bff5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a possibly underfit (depth = 3) decision tree regressor\n",
    "dt3 = tree.DecisionTreeRegressor(max_depth = 3)\n",
    "dt3.fit(X_train,y_train);\n",
    "\n",
    "# fit a well-tuned decision tree regressor using our best params from before\n",
    "dt_tuned = tree.DecisionTreeRegressor(max_leaf_nodes=best_params['max_leaf_nodes'])\n",
    "dt_tuned.fit(X_train,y_train);\n",
    "\n",
    "# and the other two models are your well-tuned Random Forest (`rf2`) and your untuned `bag` model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6a0db1f-ebc5-41ae-b35f-f92bc0a28297",
   "metadata": {},
   "source": [
    "We now explore the relationships by plotting the predictions (we'll stick with sqft for now).  \n",
    "\n",
    "**Q**: ü§î How can we generate these prediction plots?  What's the tricky detail we need to worry about?\n",
    "\n",
    "**Q**: ü§î How do tree-based models handle interactions?  Why does this make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3a5f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data frame of means to do the prediction\n",
    "means1 = X_train.mean(axis = 0)\n",
    "means_df = (means1.to_frame()).transpose()\n",
    "\n",
    "# Do the prediction across range of observed sqft\n",
    "dummy_x = np.arange(np.min(homes['sqft']),np.max(homes['sqft']),1)\n",
    "means_df  = pd.concat([means_df]*dummy_x.size,ignore_index=True)\n",
    "means_df['sqft'] = dummy_x\n",
    "yhat_rf = rf2.predict(means_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e088eae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots yhat vs. sqft at means of the other predictors\n",
    "plt.scatter(X_train['sqft'],y_train)\n",
    "plt.plot(dummy_x, bag_simple.predict(dummy_x.reshape(-1,1)), \n",
    "             label=\"bag_simple\",color='#000000')\n",
    "plt.plot(means_df['sqft'],yhat_rf,\n",
    "         label=\"rf_tuned\",color='#A51C30')\n",
    "plt.title(\"Predicted log2(price) vs. sqft from RF in train\")\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccac0131-c196-4efb-ba53-95250e213bc9",
   "metadata": {},
   "source": [
    "*Note: Harvard Colors: https://seas.harvard.edu/office-communications/brand-style-guide/color-palette"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42610ae0-2b03-4657-a69c-87e69446de33",
   "metadata": {},
   "source": [
    "**Q**: ü§î Describe what you see.  How does the RF model `log2(price)`s relationship with `sqft`? How is this different from the *simple* RF/bagging model?  Why is this not surprising?\n",
    "\n",
    "**$^*$Hint**: Think simple vs. multiple regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b691245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data frame of means for condos and separately \n",
    "# for multi-family homes to do the prediction\n",
    "means_condo = X_train[(X_train['multifamily']+\n",
    "                       X_train['singlefamily']+\n",
    "                       X_train['townhouse'])==0].mean(axis = 0)\n",
    "condo_df = (means_condo.to_frame()).transpose()\n",
    "condo_df  = pd.concat([condo_df]*dummy_x.size,ignore_index=True)\n",
    "condo_df['sqft'] = dummy_x\n",
    "\n",
    "means_multi = X_train[X_train['multifamily']==1].mean(axis = 0)\n",
    "multi_df = (means_multi.to_frame()).transpose()\n",
    "multi_df  = pd.concat([multi_df]*dummy_x.size,ignore_index=True)\n",
    "multi_df['sqft'] = dummy_x\n",
    "\n",
    "\n",
    "# Do the prediction at all observed sqft for each home type\n",
    "yhat_rf_condo = rf2.predict(condo_df)\n",
    "yhat_rf_multi = rf2.predict(multi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e98ca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots yhat vs. sqft at means of the other predictors for condos vs. multifamily homes\n",
    "plt.scatter(X_train['sqft'],y_train)\n",
    "plt.plot(condo_df['sqft'],yhat_rf_condo,\n",
    "         label='condo',color='#A51C30')\n",
    "plt.plot(multi_df['sqft'],yhat_rf_multi,\n",
    "         label='multifamily',color='#000000')\n",
    "plt.title(\"Predicted log2(price) vs. sqft from RF in train\")\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95e73305-f24d-4fc9-8cbf-8228fd0881f1",
   "metadata": {},
   "source": [
    "**Q**: ü§î Describe what you see.  How does the RF model `log2(price)`s relatoinship with `sqft`?  What does this say about any interaction effects involving `sqft`?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f0dfdee-d10e-43e7-85a1-4818efecc325",
   "metadata": {},
   "source": [
    "### Feature Importance (MDI)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09bf0786-30f5-4dea-b65a-4dbc532e2201",
   "metadata": {},
   "source": [
    "Fill in the blanks below to calculate the variable importances from the 4 untuned models above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f908913",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns[np.flip(np.argsort(dt3.feature_importances_))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2187281e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,4, figsize=(24,6))\n",
    "models = {'depth-3 tree': dt3,\n",
    "          'tuned tree': dt_tuned,\n",
    "          'bagged': bag,\n",
    "          'random forest': rf_tuned}\n",
    "\n",
    "num_features = 10 \n",
    "for i, (name, model) in enumerate(models.items()):\n",
    "    # calculation is more manual for the bagged model\n",
    "    if name == 'bagged':\n",
    "        imporances = np.array([t.feature_importances_ for t in model.estimators_]).mean(axis=0)\n",
    "    else:\n",
    "        importances = model.feature_importances_\n",
    "    order = np.argsort(importances)[-num_features:]\n",
    "    axs[i].barh(range(num_features), importances[order], tick_label=X_train.columns[order]);\n",
    "    axs[i].set_title(f\"Relative Variable Importance for {name}\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0f741a0-083e-48d6-afe8-c34789526eaf",
   "metadata": {},
   "source": [
    "**Q**: ü§î How do these variable importance measures compare for these 4 models?  Which predictor is most important in general?  How is it related to `price`? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "082dc1ce-966d-433a-be8b-d9b2f576c2a3",
   "metadata": {},
   "source": [
    "**What other approaches can be taken to measure variable importance?**\n",
    "\n",
    "One alternative for random forest:\n",
    "- Record the prediction accuracy on the *oob* samples for each tree.\n",
    "- Randomly permute the data for column $j$ in the *oob* samples, then record the accuracy again.\n",
    "- The decrease in accuracy as a result of this permuting is averaged over all trees, and is used as a measure of the importance of variable $j$ in the random forest. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9e7b4353-4eb8-4596-9860-e838f02c6040",
   "metadata": {},
   "source": [
    "This idea of re-permuting a variable and *refitting* a model to see how much more\n",
    "poorly it performs is called **permutation feature importance**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41981b36-e62d-44e6-9e21-3da394c64bc7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff73874f-efe6-4be6-8bf5-c93589930619",
   "metadata": {},
   "source": [
    "### Permutation Importance\n",
    "\n",
    "Keep in mind that when two features are correlated and one of the features is permuted, the model will still have access to the feature through its correlated feature.\n",
    "\n",
    "**Q:** ü§î What is the one glaring disadvantage to the permutation approach? ‚è≤Ô∏è\n",
    "\n",
    "**Q:** ü§î This method is often preferred to the standard, MDI feature importance approach shown above, why?\n",
    "\n",
    "SKLearn provides a built-in implementation of permutation importance that we can use to evaluate our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca63975-55e0-4e2d-a3d0-e0b37f3dd770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Calculate permutation importance\n",
    "pi_results = permutation_importance(rf, X_test, y_test, n_repeats=50, random_state=42)\n",
    "\n",
    "# Create a DataFrame to display results\n",
    "pi_data = {\n",
    "    'importance_mean': pi_results['importances_mean'],\n",
    "    'importance_std': pi_results['importances_std'],\n",
    "    'feature': X_test.columns\n",
    "}\n",
    "pi_df = pd.DataFrame(pi_data).sort_values('importance_mean', ascending=False)\n",
    "\n",
    "# Sort by importance for plotting\n",
    "pi_df_sorted = pi_df.sort_values('importance_mean', ascending=True).reset_index(drop=True)\n",
    "pi_df_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3ff7ac-74f5-4bf3-870f-ada994a9fc54",
   "metadata": {},
   "source": [
    "The results show:\n",
    "- importance_mean: Average decrease in model score when the feature is permuted\n",
    "- importance_std: Standard deviation of the importance across permutations\n",
    "- feature: Name of the feature being evaluated\n",
    "\n",
    "Higher importance values indicate features that, when randomly shuffled, lead to larger decreases in model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb5c8f1-ee78-47f6-b32c-5eb3f0a9d64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance barplot with error bars\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(pi_df_sorted)), pi_df_sorted['importance_mean'],\n",
    "         xerr=pi_df_sorted['importance_std'],\n",
    "         capsize=5, alpha=0.8, color='#A51C30')  # Harvard Crimson color!\n",
    "plt.yticks(range(len(pi_df_sorted)), pi_df_sorted['feature'])\n",
    "plt.xlabel('Permutation Importance (decrease in $R^2$ score)')\n",
    "plt.title('Permutation Feature Importance with Standard Deviation')\n",
    "\n",
    "# Add grid for easier comparison\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30854cd0-1405-46fc-bbf5-b107124c33b9",
   "metadata": {},
   "source": [
    "**Q**: ü§î How do the permutation importance measures compare to the default variable importance in the random forest?  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39b7e5f8-847f-46a1-bee7-5c5875d93f24",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "105fb214-4da1-4ddf-bcf9-24faab20e25c",
   "metadata": {},
   "source": [
    "Variable Importance is great! It tells you what features are important in shaping the model predictions.\\\n",
    "But what is missing?\n",
    "- It does not give any measure for *how* the predictors are related to the response (positive, negative, quasi-linear, curved, interactions, etc.).\n",
    "- This is where the parametric model wins out! Inference and interpretations are much easier and is the whole point of such models.\n",
    "\n",
    "**Q:** ü§î What can we do to measure these relationships in a machine learning or nonparametric model? What did we do with k-NN? \n",
    "\n",
    "Well, we could just plot the predictions! Easy with 1 predictor, but what if we have hundreds?\n",
    "\n",
    "How do I extract the relationship between a given predictor and the response when, in the model, it is embedded in the context of the other predictors?\\\n",
    "We could hold all the other variables constant!\n",
    "\n",
    "What needs to be done algorithmically to put this in practice?\n",
    "Set the other predictors equal to *something* and only vary the predictor of interest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "071d3cc8-12ef-43c8-876f-f69f04e93260",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69b16ab2-eae1-43c4-a865-5d5e156d6fe2",
   "metadata": {},
   "source": [
    "### LIME \n",
    "\n",
    "The above stratagies attempt to give us a **global** explanation of our model's predictions.\n",
    "But we may be interested in a **local** explanation. That is, why did our blackbox model give *a particular* observation the prediction it did?\n",
    "\n",
    "A method called **LIME** was proposed in the paper [\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier](https://arxiv.org/pdf/1602.04938v3.pdf), and it attempts to do just this. So what is LIME?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "430a1db6-adc2-4566-8896-5107048140a1",
   "metadata": {},
   "source": [
    "**Locally**: The explanation should be able to explain how the model behaves for individual observations.\\\n",
    "**Interpretable:** The explanation must be easy to understand by humans (but may depend on the target audience).\\\n",
    "**Model-agnostic:** The method should be able to explain any model.\\\n",
    "**Explanations:** self-explanatory üòÅ. This is the whole goal!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "514f35fb-4fc9-410b-9c6e-ed6716e8bc5d",
   "metadata": {},
   "source": [
    "The approach is to use a directly interpretable model (e.g., a linear model) to help explain a model that is not directly interpretable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02f6d716-e7aa-48ef-b3eb-420d5d2f4a6e",
   "metadata": {},
   "source": [
    "**LIME ALGORITHM**\n",
    "1. Select your observation of interest for which you want to have an explanation of its black box prediction.\n",
    "2. Randomly generate points all over the feature space (sample X values from a Normal distribution inferred from the training set)\n",
    "3. Get the black box predictions for these new points.\n",
    "4. Weight each of the generated points based on their proximity to the point of interest using a kenerl function (e.g., exponential, Gaussian, etc.)\n",
    "5. Fit a *weighted*, interpretable model on the synthetic data and the original model's predictions.\n",
    "6. Explain the prediction by interpreting the local model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c8acd0c-cf58-4a87-81fb-04784e61218d",
   "metadata": {},
   "source": [
    "This method can be applied to tabular data, text, and images!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa845323-1b3c-4e7a-a7df-df9ee721cc83",
   "metadata": {},
   "source": [
    "**Toy Example: Classification Problem**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68145c3a-b953-4872-969c-84c2f69daf33",
   "metadata": {},
   "source": [
    "<img src='fig/LIME_plot.png' width='400px'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "375ea1de-1a35-4175-b98d-eee971eaa4af",
   "metadata": {},
   "source": [
    "In the plot above, the colored regions represent either side of the original model's decision boundary.\\\n",
    "The large red marker is our point of interest.\\\n",
    "The other points are the generated data. Their color represents the *original* model's predictions and their size represents their weight based on their distance from the point of interest.\\\n",
    "The dashed line is the predictions of the interpretable surrogate model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf31c920-a5b6-42b5-a121-6536e78fac0b",
   "metadata": {},
   "source": [
    "The **objective** can be expressed as:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36f045cc-964e-4123-9f3f-3280dffe8a42",
   "metadata": {},
   "source": [
    "<img src='fig/LIME_eq.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06f3860a-5afd-44b6-93d4-4f2633c0e8c3",
   "metadata": {},
   "source": [
    "The explanation model for observation $x$ is the model $g$ (e.g., logistic regression model) that minimizes loss $L$ (e.g., cross-entropy), which measures how close the explanation is to the prediction of the original model $f$ (e.g., random forest), while the model complexity $\\Omega$ is kept low (e.g., prefer fewer features).\\\n",
    "$G$ is the family of possible explanations.\\\n",
    "$\\pi_x$ is a proximity measure defining a neighborhood around instance $x$ we consider for an explanation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4136e618-c88f-4853-936b-ac705ce4b88d",
   "metadata": {},
   "source": [
    "There are several LIME implementations for Python. We'll be using one simply called [lime](https://github.com/marcotcr/lime)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29952cda-faa6-4ce1-b2c3-52c9d3b3d419",
   "metadata": {},
   "source": [
    "Because we're working with tabular data we'll be using [LimeTabularExplainer](http://gael-varoquaux.info/interpreting_ml_tuto/content/02_why/04_black_box_interpretation.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77de679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976a53b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5742017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "explainer = LimeTabularExplainer(X_train.values,\n",
    "                                 feature_names=X_train.columns,\n",
    "                                 class_names = [0,1],\n",
    "                                 mode='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaef62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 109\n",
    "\n",
    "exp = explainer.explain_instance(X_train.values[idx], \n",
    "                                 rf.predict, \n",
    "                                 num_features = 10)\n",
    "\n",
    "print('Observation #: %d' % idx)\n",
    "print('Predicted Price (in thousands) =', 2**(rf.predict(X_train)[idx]))\n",
    "print('Actual Price (in thousands): %s' % 2**(y_train.iloc[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f276126",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the results\n",
    "exp.as_pyplot_figure();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa28dfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the observation number and see what changes.\n",
    "idx = 209\n",
    "\n",
    "exp = explainer.explain_instance(X_train.values[idx], \n",
    "                                 rf.predict, \n",
    "                                 num_features = 10)\n",
    "\n",
    "print('Observation #: %d' % idx)\n",
    "print('Predicted Price (in thousands) =', 2**(rf.predict(X_train)[idx]))\n",
    "print('Actual Price (in thousands): %s' % 2**(y_train.iloc[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b19a29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot the results\n",
    "# exp.as_list()\n",
    "exp.as_pyplot_figure();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbac79b7-0246-4ac7-be7e-0f42c186b246",
   "metadata": {},
   "source": [
    "We also have a `show_in_notebook` methode but the pyplot figure one above is more concise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca30a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(show_table=True, show_all=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef8f205c-05ab-446b-bdb3-e5cb09a4a877",
   "metadata": {},
   "source": [
    "You can also print the explanation as list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c873e173",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.as_list()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9d20c40-839b-4f60-986f-220cba3e193c",
   "metadata": {},
   "source": [
    "**Q:** ü§î Interpret the LIME results above.  Do they agree with the other interpretations for the random forest model seen so far?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e3d40a99-10aa-441e-b906-39d08b5d0d4f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <strong>üèãüèª‚Äç‚ôÇÔ∏è TEAM ACTIVITY:</strong> Inspect the Worst Prediction</div>  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97438d45-8dee-4be3-a1a2-b9da6df1b1eb",
   "metadata": {},
   "source": [
    "**Instruction:**\n",
    "- Which observation in the test data does the random forest get *most wrong*?\n",
    "    - Think about how you would determin this\n",
    "- Use LIME to interpret this bad prediction.\n",
    "    - Do we have any insight into what is driving the mistake or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09106db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5853b1f7-2da2-4329-b2eb-d6febeff389c",
   "metadata": {},
   "source": [
    "#### Potential Issues\n",
    "\n",
    "- The surrogate is fit to **randomly generated data**. And so interpretations can be *unstable*, changing with each run.\n",
    "- The local approximation is highly sensitive to the choice of **kernel width**.\n",
    "\n",
    "When LIME was initially proposed, the kernel used to define the neighborhood near the point of interest was selected using heuristics.\\\n",
    "\n",
    "<img src='fig/SHAP_kernel.png' width='700px'>\n",
    "\n",
    "<p style=\"font-size:11px\">Image by <a href=\"https://towardsdatascience.com/lime-explain-machine-learning-predictions-af8f18189bfe\">Giorgio Visani</a></p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "946cbd9f-4ef6-4036-84ea-e23439f0ca5d",
   "metadata": {},
   "source": [
    "Though there has been recent work on LIME's instability and sensitivity to kernel width. [OptiLIME](https://arxiv.org/pdf/2006.05714.pdf) proposes a more principled way.\\\n",
    "They have an open-source implementation on [Github](https://github.com/giorgiovisani/lime_stability/tree/master/OptiLIME)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0d66cd8-f547-4e1b-b2f6-5d7d8909decd",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We looked at how we can use prediciton plots, variable importance, permutation importance, and LIME to explain machine learning models in model-agnostic way.\n",
    "\n",
    "But there will always remain a tension between explainability/interpretability and accuracy of statistical and machine learning models.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28613861-75eb-4da5-9117-994a59e59a41",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Interpretable ML: https://christophm.github.io/interpretable-ml-book\n",
    "\n",
    "ELI5: https://eli5.readthedocs.io/en/latest/index.html\n",
    "\n",
    "LIME: https://github.com/marcotcr/lime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9093cd-b39b-45cc-b3bf-91eea0f85d4f",
   "metadata": {},
   "source": [
    "üåà **The End**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c6645a-b2b1-4132-bc57-b36431474791",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
