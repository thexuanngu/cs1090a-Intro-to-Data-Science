{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5,
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# \u003cimg style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"\u003e CS109A Introduction to Data Science \n",
                "\n",
                "## Lab 3: kNN and Linear Regression\n",
                "\n",
                "**Harvard University**\u003cbr/\u003e\n",
                "**Fall 2024**\u003cbr/\u003e\n",
                "**Instructors**: Pavlos Protopapas and Natesh Pillai\u003cbr/\u003e\n",
                "\u003chr style='height:2px'\u003e"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\u003ca id=top\u003e\u003c/a\u003e\n",
                "## Learning Objectives\n",
                "\n",
                "This will mark our first foray into modeling! Specifically, we'll look at linear regression  and k-nearest neighbors (kNN) regression.\\\n",
                "The goal of this lab is to set you up for success on HW2. To that end we'll look at:\n",
                "\n",
                "- Inspecting your dataset. e.g., what are the data types? are my predictors categorical or quantitative? etc.\n",
                "- Setting up our regression problem:\n",
                "    - what do we want to predict? (i.e., the what is the response variable?)\n",
                "    - selecting a predictor variable for your regression model\n",
                "- Train-Test Split\n",
                "- Fitting a simple, single variable regression model:\n",
                "    - linear regression\n",
                "        - accessing the learned parameters ($\\beta_0$ \u0026 $\\beta_1$)\n",
                "        - checking model assumptions with residual plots\n",
                "    - kNN\n",
                "        - selecting the optimal $k$\n",
                "        - visualizing the train/test loss as a function of $k$\n",
                "- evaluating the models with MSE and $R^2$ metrics\n",
                "- plotting predictions\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Using the `%load` Command\n",
                "Cells with `# %load` allow you to quickly load pre-written code.\n",
                "\n",
                "**You'll get the most out of the lab if attempt to write the code yourself.** But if you get stuck or feel like you are falling behind:\n",
                "\n",
                "1. Uncomment the `# %load` line (`Ctrl + /` or `Cmd + /` on macOS).\n",
                "2. Run the cell.\n",
                "\n",
                "This brings in the correct code for that cell."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/imports.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## The Dataset\n",
                "\n",
                "**The dataset is provided in the file** `data/bikeshare.csv`.\n",
                "\n",
                "Our dataset was collected by the Capital Bikeshare program in Washington D.C (*source: [UCI repository](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset)*). It contains over two years of data on the total number of bike rentals per day, as well as 10 attributes describing the day and its weather (see below for a description). \n",
                "\n",
                "The task is to build a regression model to **predict the total number of bike rentals in a given day** (known as the response variable) based on attributes about the day (known as the 'features' or 'predictors'). Such a forecasting model would be useful in planning the number of bikes that need to be available in the system on any given day, and also in monitoring traffic in the city.\n",
                "\n",
                "**Description of variables**\n",
                "\n",
                "- season (1:winter, 2:spring, 3:summer, 4:fall) \n",
                "- month (1 through 12, with 1 denoting Jan)\n",
                "- holiday (1 = the day is a holiday, 0 = otherwise, extracted from https://dchr.dc.gov/page/holiday-schedules)\n",
                "- day_of_week (0 through 6, with 0 denoting Sunday)\n",
                "- workingday (1 = the day is neither a holiday or weekend, 0 = otherwise)\n",
                "- weather \n",
                "    - 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
                "    - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
                "    - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
                "    - 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n",
                "- temp (temperature in Celsius)\n",
                "- atemp (apparent, or relative outdoor, or real feel temperature, in Celsius)\n",
                "- humidity (relative humidity)\n",
                "- windspeed (wind speed)\n",
                "------------------------\n",
                "- **count** (response variable i.e. total number of bike rentals on the day)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Take a look at the data!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/inspect.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Choosing a predictor"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "If we can only use one feature for predicting `count`, which should we choose?"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "for example, what is the relationship between `humidity` and `count`?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/quick_scatter.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Why not look at all features?**"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "First get a list of all the potential predictor columns and see how many there are"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "# %load snippets/predictor_cols.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, with the help of `plt.subplots()`, create a grid of scatter plots showing the relationship between each potential predictor and `count`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/scatter_subplots.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Seaborn's `pairplot` method allows us to do something very similar with very little code.\n",
                "\n",
                "But with a DataFrame with $M$ columns you get a huge plot of all $M\\times M$ relationships! We can use the `x_vars` and/or `y_vars` arguments to list only those variables to include on each axis."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/pairplot.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Another approach is to visualize the correlation between each predictor.\n",
                "\n",
                "We can get the correlation coefficients with the `corr()` DataFrame method and then use Seaborn's `heatmap()` to vizualize the values."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/corr_heatmap.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Q:** ü§î Is there a way we could change the default heatmap to make it easier to interpret?"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "It looks like `temp` is the winner (at least if we plan to use a linear model)."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### We have our feature. Now what?"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now let's separate our chosen feature and the response variable into `x` and `y`.\n",
                "\n",
                "You may want to check the data types of your new variables so you know what you're working with."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/x_and_y.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Check the shapes!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/shape_check.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "SKLearn models expect the predictors, $X$, to be in the form of a matrix. That is, there should be a nexplicit 2nd dimension. One way to handle this requirement is to use the double-brackets when indexing into your DataFrame to so you get a DataFrame back (and not a Series). If you are dealing with a 1D Numpy array, you can add a second dimension with the `reshape` or `expand_dims` methods."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Train-Test Split"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We might be tempted to jump right to model fitting, but hold your horses! ‚úãüêé\n",
                "\n",
                "If we use all of our data for training we are then forced to *evaluate* the model on the same data it was trained on!\n",
                "\n",
                "But we want some sense of how well the model will **generalize**. That is, how well it performs on previously unseen data.\n",
                "\n",
                "A low loss on the training data does not gaurantee good generalization. The model may have fit to the idiosyncrasies of our *particular* training data (noise, outliers, etc.) that are not representative of the wider population of potential data points. This would be a case of the dreaded **overfitting**.\n",
                "\n",
                "So let's split our dataset into two pieces. A **train set** which we will use to fit the model, and a **test set** used for evaluating the model after fitting."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We'll be using a lot of classes and methods from the [sklearn](https://scikit-learn.org/stable/index.html) library both for our models themselves as well as helper functions for splitting and metrics. "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "SKLearn functions and classes are grouped into different modules within the library. Search the [sklearn documentation](https://scikit-learn.org/stable/modules/classes.html) for \"train test split\" in order to see what module the relevent function resides in. This will tell us how we can import it! You can of course all read the full documentation on the function itself there."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/import_ttsplit.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "It is important to shuffle the data (`shuffle=True` by default) to ensure a random sample for the test data.\n",
                "\n",
                "Setting the `random_state` allows for reproducible results, ensuring we get the same split each time the code is run."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/split_x_y.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Again, inspecting shapes can be helpful if you aren't sure you named the output correctly."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/split_shapes.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can also visually inspect that the split was random.\n",
                "\n",
                "Create a scatter plot of temp vs count where train and test data are given different colors.\n",
                "\n",
                "You should always use a **legend** to designate the meaning of each color. Also, adjusting the **alpha** value can be helpful to see where points are densly clustered."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/split_scatter.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Simple Linear Regression\n",
                "\n",
                "Import the `LinearRegression` class from sklearn's `linear_model` module."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/import_linreg.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Fit a linear regression (on the train data of course!)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/fit_linreg.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Inspect the learned model parameters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/linreg_params.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can get the same model parameters using the linear algebra Pavlos showed in lecture.\\\n",
                "$$\\beta = (X^{\\top}X)^{-1}X^{\\top}Y$$"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Just remember that we actually need to augment our $x$ vector into a matrix with a column of ones if we want an intercept ($\\beta_0$) term (consult the lecture 5 slides for details).\n",
                "\n",
                "We can can store the augmented `x_train` as `X`. There are many ways to do this. One is to use a combination of `np.ones_like()` and `np.hstack()`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/aug_x.py \n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Then it is just a matter of performing the matrix operations shown in the $\\LaTeX$ above!\n",
                "\n",
                "Compare the resulting $\\beta$ values to the ones returned by SKLearn."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/linreg_linalg.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Model Evaluation\n",
                "\n",
                "Let's calculate the train MSE. This will require predictions. You get these by calling the model's `predict` method on the appropriate data.\n",
                "\n",
                "First, calculate the MSE by hand..."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/mse_scratch.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "...then again but this time with the help of sklearn's `mean_squared_error` method from the `metrics` module."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/mse_skl.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now calculate the test MSE!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/mse_test.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Are these good or bad MSEs? It is hard to make that kind of judgement given that they depend on the scale of the response variable.\n",
                "\n",
                "Instead, we can use the **$R^2$ score** which compares out model to a baseline model which always predits the mean response value."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Recall that $R^2 = 1 - \\frac{\\text{unexplained variation}}{\\text{total variation}}$\n",
                "\n",
                "Calculate the $R^1$ score on the test data. First by hand..."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/r2_scratch.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "...and again using the `r2_score` metric from sklearn."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/r2_skl.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Sklearn models ('estimators' in their parlance) have a standardized API which include common methods like `fit` and `predict`.\n",
                "\n",
                "There is also a `score` method which will return a (model dependant) metric when given an $x$ and $y$. This metric happens to be the $R^2$ for both the `LinearRegression` and `KNeighborsRegressor` estimators.\n",
                "\n",
                "Under the hood it is making predictions and then comparing those to the true $y$ values."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Use your model's `score` to get the $R^2$ score on the test data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/linreg_score.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### D.R.Y: Don't Repeat Yourself"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's make a function to help calculate metrics for our models since we will do this multiple times in the notebook.\n",
                "\n",
                "Complete the `get_metrics` function so it returns a dictionary containing the requested information about the input model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/metrics_helper.py\n",
                "def get_metrics(model, name: str, data: tuple) -\u003e dict:\n",
                "    '''\n",
                "    Parameters\n",
                "    ----------\n",
                "    model : (sklearn estimator) This is your fitted sklearn model\n",
                "    name : (str) A name to represent your model\n",
                "    data : (tuple) contains train and test split for x and y\n",
                "\n",
                "    Returns:\n",
                "    metrics : (dict) should have entries for:\n",
                "             name, train_mse, test_mse, r2_train, and r2_test\n",
                "    '''\n",
                "    x_train, x_test, y_train, y_test = data\n",
                "    # your code here\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We'll keep track of our models' results using a list of dictionaries.\n",
                "\n",
                "Use `get_metrics` to calculate the metrics for your linear regression model and add it to a list called `results`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/linreg_results.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This also allows us to display the results in a nice DataFrame. OooOoo!\n",
                "\n",
                "Use `results` to create a DataFrame."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/linreg_results_df.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "####  Visualizing our model\n",
                "\n",
                "The metrics above give us information about the model's performance. But a picture can also help our understanding.\n",
                "\n",
                "Let's create a plot showing the train data, test data, and model predictions *for all values of the predictor in the range of the dataset*."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "First, use `np.linspace` to create an array of equally spaced values spanning the range of temperatures observed in the entire dataset. Store these values in `x_lin`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/x_lin.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now the actual plot!\n",
                "\n",
                "Overlay the model's prediction line on top of a scatter plot of the train and test data (you can recycle that plotting code form earlier)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/linreg_pred_plot.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### What about the assumptions we make when using a linear model?\n",
                "\n",
                "Pavlos told us in lecture that two of our assumptions when using a linear model are that (1) the variance of the residuals is constant and (2) the residuals are normally distributed. Let's check those assumptions."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "First, we'll check for constant variance of the residuals. Recall that the residuals are the difference between the true value and the predicted value.\n",
                "\n",
                "Calculated the residuals on the test data and store them in `resids`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/resids.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Create a scatter plot of the residuals against the predictor. It can be helpful to add a horizontal line at 0 "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/resids_plot.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "There seems to be a slight 'hump' in the middle..."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, check that residuals are normally distributed. We can use a histogram for this."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/resids_dist.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Hmm, not quite symmetrical either.\n",
                "\n",
                "If we look back at our plot of the training data we can see there is a slight curve in the trend. We might have expected this: bike riding increases as the tempature increases, but only up to a point. At some point the heat makes people less inclined to go for a ride! üö¥‚Äç‚ôÄÔ∏èüåû\n",
                "\n",
                "Perhaps a non-linear model like kNN may be better suited to model the relationship between `temp` and `count`."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### kNN\n",
                "\n",
                "Fit a kNN model using the `sklearn.neighbors.KNeighborsRegressor` class. To start, let's use `n_neighbors=10`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/fit_k10.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Unlike linear regression, fitting kNN does not calculate any parameters. It simply stores the training data in an efficient way for calculating pair-wise distances at prediction time.\n",
                "\n",
                "$k$ is a *hyperparamter*. Hyperparameters are *not* learned from the data. Instead, they are set by the practitioner (like you!) and define the learning/prediction algorithm that will follow. We'll talk more about hyperparameters as the course continues."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Metrics"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Use `get_metrics` to calculate the kNN model's metrics, append them to `results`, and display `results` as a DataFrame."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/k10_results.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Choosing $k$\n",
                "\n",
                "OK, great! But $k=10$ was an arbitrary choice. Why not choose the $k$-value that minimizes the test MSE?\n",
                "\n",
                "We'll fit kNN regressors on a range of k values and inspect the loss as a function of $k$.\n",
                "\n",
                "Let's try all 1 through 100 for our potential $k$ values. We should store the train and test MSEs for each model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/all_k_mses.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now let's plot the the losses as a function of $k$ and identify the best $k$-value based on the test loss."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/mse_vs_k_plot.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Q:** Might we be justified in choosing a $k$ that does not produce the lowest test score? ü§î\n",
                "\n",
                "**Q:** Do you have any reason to be skeptical about using the test score to determine the best $k$? ü§î"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Final metric comparison"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Fit the best kNN model, calculate and append its metrics to `results`, and again display all model results as a DataFrame."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/fit_best_k.py\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/best_k_results.py\n",
                ""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/final_results.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Plotting $k$-NN Predictions\n",
                "\n",
                "Now let's plot our kNN model's predictions. This can be trickier than plotting the linear regression predictions!\n",
                "\n",
                "You will again need to use something like `np.linspace` to generate inputs for the predictions. Remember that the kNN predictions should look like a step function; there should be no diagonal sections. If you see diagonal portions of the prediction line then you are not using a sufficient number of points in your linspace.\n",
                "\n",
                "Feel free to use the `x_lin` you defined earlier when making the linear regression prediction plots. You can also reuse the data scatter plot code (we probably should have written a function!)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/plot_best_k_pred.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Prediction plots for multiple choices of $k$"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "How would you go about creating a visualization comparing the predictions of several different kNN models, each using different value of $k$? For example, let's try values 1, 2, 4, 8, 16, 32, 64, and 128. "
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "First, create a list of candidate $k$ values called `ks`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/candidate_ks.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, fit a KNN model for each candidate $k$ and store their predictions in a list called `all_preds`.\n",
                "\n",
                "**Q:** What should we predict on? ü§î (Remember that we are making a plot!)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/get_all_k_preds.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The number of candidate $k$ values is even, so let's create a grid of prediction plots with 2 columns.\n",
                "\n",
                "(Reviewing the manual pairplot we did way back in the beginning of the notebook may help here)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "# %load snippets/plot_all_k_preds.py\n",
                ""
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As expected, for low values of $k$ the prediction line is complex and jagged. It smooths at as $k$ increase because more neighbors are considered for each prediction."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Bonus:** Try changing the `random_state` in the call to `train_test_split` and see how it affects your results, both in regards to the metrics and the choice of best $k$. It seems our results are sensitive to the random test split we received! This motivates the idea of **cross-validation** which will be covered next week!"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**The End!** üéâ"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                ""
            ]
        }
    ]
}
